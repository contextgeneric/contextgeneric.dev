+++

title = "Unleashing Extensible Records and Variants in Rust with CGP v0.4.2"

description = ""

authors = ["Soares Chen"]

+++

# Overview

I’m excited to announce the release of **CGP v0.4.2**, a major milestone that significantly expands the expressive power of generic programming in Rust. With this release, CGP introduces full support for **extensible records and variants**, unlocking a range of new capabilities for developers working with highly modular and reusable code.

Extensible records and variants allow developers to write code that operates on *any struct containing specific fields* or *any enum containing specific variants*, regardless of their complete definition. This makes it possible to write truly generic and flexible logic that is decoupled from rigid type definitions.

In earlier versions, CGP already offered a foundational feature through the `HasField` trait, which made it possible to *read* a field from any struct that included it. With version 0.4.2, this functionality is dramatically extended. Not only can you now read fields, but you can also *construct* values onto these fields in a type-safe manner. More importantly, the same level of extensibility is now available for enums, enabling operations over variants in a similarly generic fashion.

This advancement introduces two powerful programming patterns that are now possible with CGP:

1. **Extensible Builder Pattern**: This pattern allows for modular construction of structs from independent sub-structs, each contributing specific fields. It enables highly composable and decoupled design in data construction.

2. **Extensible Visitor Pattern**: This pattern enables the modular deconstruction of enums, allowing independent components to handle different variants without requiring full knowledge of the entire enum definition. This effectively enables a modularized version of the _visitor pattern_, by allowing new variants to be handled by extensible visitors.

For readers coming from more advanced programming languages, this development effectively brings the power of [**datatype-generic programming**](https://wiki.haskell.org/index.php?title=Generics), [**structural typing**](https://en.wikipedia.org/wiki/Structural_type_system), [**row polymorphism**](https://book.purescript.org/chapter4.html) and [**polymorphic variants**](https://ocaml.org/manual/5.1/polyvariant.html) to Rust. These are advanced type system features commonly found in languages like Haskell, PureScript and OCaml, and their availability in CGP represents a major leap in what is possible with the type system in Rust.

In addition, CGP v0.4.2 introduces support for safe **upcasting and downcasting between enums** that share a common subset of variants. This provides a foundation for writing extensible and evolvable APIs that remain compatible across different layers of abstraction or across independently maintained modules.

# Table of Contents

In this post, we will go through a high level overview of each feature introduced, together with some practical examples of how they can be used.


# Safe Enum Upcasting

Let’s begin by looking at how CGP enables safe upcasting between enums. Imagine you have the following enum definition:

```rust
#[derive(HasFields, ExtractField, FromVariant)]
pub enum FooBarBaz {
    Foo(u64),
    Bar(String),
    Baz(bool),
}
```

You may also have a different enum, defined elsewhere, that represents only a subset of the variants:

```rust
#[derive(HasFields, ExtractField, FromVariant)]
pub enum FooBar {
    Foo(u64),
    Bar(String),
}
```

With CGP v0.4.2, it is now possible to *upcast* a `FooBar` value into a `FooBarBaz` value in fully safe Rust:

```rust
let foo_bar: FooBar = FooBar::Foo(1);
let foo_bar_baz: FooBarBaz = foo_bar.upcast(PhantomData::<FooBarBaz>);
assert_eq!(foo_bar_baz, FooBarBaz::Foo(1));
```

This operation works by leveraging the derived CGP traits `HasFields`, `ExtractField`, and `FromVariant`. As long as the source enum’s variants are a subset of the target enum’s, CGP can automatically generate the logic required to lift the smaller enum into the larger one.

A particularly powerful aspect of this design is that the two enums do not need to know about each other. They can be defined in entirely separate crates, and the trait derivations are completely general. You don’t need to define any enum-specific conversion traits. This makes it possible to build libraries of reusable variant groups and compose them freely in application code.

# Safe Enum Downcasting

In the reverse direction, CGP also supports *safe downcasting* from a larger enum to a smaller one that contains only a subset of its variants. Using the same `FooBarBaz` and `FooBar` enums, the following example demonstrates how this works:

```rust
let foo_bar_baz: FooBarBaz = FooBarBaz::Bar("hello".to_owned());
let foo_bar: Result<FooBar, _> = foo_bar_baz.downcast(PhantomData::<FooBar>);
assert_eq!(foo_bar.ok(), Some(FooBar::Bar("hello".to_owned())));
```

Like `upcast`, this `downcast` method relies on the same set of derived CGP traits and works for any pair of compatible enums. The operation returns a `Result`, where the `Ok` variant contains the downcasted value, and the `Err` variant carries the unhandled remainder of the original enum.

In the example above, we use `.ok()` to simplify the comparison, but in practice, the `Err` case contains useful data that can be further examined or downcasted again, which leads us to one of the most innovative aspects of CGP’s enum handling.

## Safe Exhaustive Downcasting

One of the unique capabilities CGP provides is the ability to *exhaustively downcast* an enum, step by step, until all possible variants are handled. This pattern becomes especially useful when working with generic enums in extensible APIs, where the concrete enum definition is unknown or evolving.

To demonstrate this, suppose we define another enum to represent the remaining `Baz` variant:

```rust
#[derive(HasFields, ExtractField, FromVariant)]
pub enum Baz {
    Baz(bool),
}
```

Now, the combination of `FooBar` and `Baz` covers the entire set of variants from `FooBarBaz`. We can use this setup to exhaustively handle all possible cases, while staying entirely within the bounds of safe Rust:

```rust
let foo_bar_baz: FooBarBaz = FooBarBaz::Foo(1);

let result: String = match foo_bar_baz.downcast(PhantomData::<FooBar>) {
    Ok(foo_bar) => {
        format!("downcasted to FooBar: {:?}", foo_bar)
    }
    Err(remainder) => match remainder.downcast_fields(PhantomData::<Baz>) {
        Ok(baz) => {
            format!("downcasted to Baz: {:?}", baz)
        }
        Err(remainder) => remainder.finalize_extract(),
    },
};
```

In this example, we first attempt to downcast into `FooBar`. If that fails, the remainder is passed to `downcast_fields`, which attempts to extract the `Baz` variant. Finally, if all known variants have been handled and nothing remains, we call `finalize_extract`, which is only valid on an "empty" enum. This indicates that all variants have been successfully matched, and the code is guaranteed to be exhaustive.

The `downcast_fields` method is intended for intermediary or "partial" enums that contain only the unhandled variants, while `finalize_extract` is used when no variants are left. This closely models the behavior of Rust’s `!` (never) type and ensures that every path in your code is covered without panics or unsafe code.

At first glance, this approach may appear more complex than simply matching against the original enum directly. However, its true strength lies in its **generality**. With CGP’s downcasting mechanism, you can pattern match over generic enum types without knowing their full structure in advance. This enables highly extensible and type-safe designs where variants can be added or removed modularly, without breaking existing logic.


# Safe Struct Building

Just as CGP enables safe, composable deconstruction of enums, it also brings **extensible construction** to structs. This is achieved through a form of structural merging, where smaller structs can be incrementally combined into larger ones. The result is a flexible and modular approach to building complex data types, well-suited for highly decoupled or plugin-style architectures.

To illustrate this, let’s take the example of a `FooBarBaz` struct:

```rust
#[derive(HasFields, BuildField)]
pub struct FooBarBaz {
    pub foo: u64,
    pub bar: String,
    pub baz: bool,
}
```

Suppose we also define two smaller structs—`FooBar` and `Baz`—each containing a subset of the fields in `FooBarBaz`:

```rust
#[derive(HasFields, BuildField)]
pub struct FooBar {
    pub foo: u64,
    pub bar: String,
}

#[derive(HasFields, BuildField)]
pub struct Baz {
    pub baz: bool,
}
```

With CGP, we can now construct a `FooBarBaz` value in a modular and extensible way, by composing these smaller building blocks:

```rust
let foo_bar = FooBar {
    foo: 1,
    bar: "bar".to_owned(),
};

let baz = Baz { baz: true };

let foo_bar_baz: FooBarBaz = FooBarBaz::builder()
    .build_from(foo_bar)
    .build_from(baz)
    .finalize_build();
```

Here’s what’s happening: The `builder()` method on `FooBarBaz` initiates a *partial builder*, an intermediate structure that initially contains none of the target fields. Each call to `build_from` takes a struct that contributes one or more of the remaining fields and returns a new builder with those fields filled in. Once all required fields have been supplied, the `finalize_build()` method consumes the builder and produces a fully constructed `FooBarBaz` instance.

Just like enum upcasting and downcasting, the struct builder is implemented entirely in **safe**, **panic-free** Rust. There’s no runtime reflection or unsafe code involved. The only requirement is that the participating structs must have compatible fields and derive the CGP-provided traits `HasFields` and `BuildField`.

Moreover, this system is completely decoupled from specific struct definitions. The individual component structs—`FooBar`, `Baz`, and `FooBarBaz`—can be defined in separate crates, with no awareness of each other. Once the CGP traits are derived, they become interoperable through structural field compatibility alone.

While this example may seem trivial—after all, constructing `FooBarBaz` directly is straightforward—it serves as a foundation for much more powerful generic abstractions. As you’ll see in the upcoming sections, the builder pattern opens the door to writing highly reusable, type-safe logic that can construct **generic structs** without ever referencing their concrete types. This makes it possible to write libraries or plugins that contribute data to a shared structure without tight coupling or dependency on a central type definition.

# Extensible Records

To understand how extensible records enable modular builders, let’s explore a practical use case: constructing an application context from configuration inputs.

Imagine we’re building an API client for our application. The application context needs to include an SQLite database connection and an HTTP client. A typical way to model this in Rust would be to define a struct like the following:

```rust
#[cgp_context]
pub struct App {
    pub sqlite_pool: SqlitePool,
    pub http_client: Client,
}
```

This `App` struct holds a [`SqlitePool`](https://docs.rs/sqlx/latest/sqlx/sqlite/type.SqlitePool.html) from the `sqlx` crate, and an HTTP [`Client`](https://docs.rs/reqwest/latest/reqwest/struct.Client.html) from `reqwest`. To construct this context, we might implement a `new` function as follows:

```rust
impl App {
    pub async fn new(db_path: &str) -> Result<Self, Error> {
        let http_client = Client::new();
        let sqlite_pool = SqlitePool::connect(db_path).await?;

        Ok(Self {
            http_client,
            sqlite_pool,
        })
    }
}
```

This constructor is asynchronous and returns a `Result<App, Error>`. It creates a default `Client` using `reqwest`, connects to the database using the provided path, and assembles both into an `App` struct.

## Adding AI Capabilities to `App`

At this point, the constructor looks simple. But in a real-world setting, it’s rarely that clean. Suppose the product team now wants to integrate AI capabilities into the application. To support this, we decide to use an LLM service like ChatGPT and extend the `App` struct accordingly:

```rust
#[cgp_context]
pub struct App {
    pub sqlite_pool: SqlitePool,
    pub http_client: Client,
    pub open_ai_client: openai::Client,
    pub open_ai_agent: Agent<openai::CompletionModel>,
}
```

In this updated version, we introduce two new fields: `open_ai_client`, which is used to communicate with the OpenAI API, and `open_ai_agent`, which encapsulates a configured agent that can perform conversational tasks using a model like GPT-4o using [`rig`](https://docs.rs/rig-core/latest/rig/index.html).

The `new` constructor must now also handle the initialization logic for these fields:

```rust
impl App {
    pub async fn new(db_path: &str) -> Result<Self, Error> {
        let http_client = Client::new();
        let sqlite_pool = SqlitePool::connect(db_path).await?;
        let open_ai_client = openai::Client::from_env();
        let open_ai_agent = open_ai_client.agent("gpt-4o").build();

        Ok(Self {
            http_client,
            sqlite_pool,
            open_ai_client,
            open_ai_agent,
        })
    }
}
```

Here, we initialize the OpenAI client using environment variables, and then build an agent configured for the `gpt-4o` model. These values are added alongside the existing HTTP and database clients.

## From Simple to Complex

Even with these additions, our constructor remains relatively manageable. However, as often happens in production, the requirements grow—and so does the configuration logic. Let’s imagine a more realistic version of this `new` function:

```rust
impl App {
    pub async fn new(
        db_options: &str,
        db_journal_mode: &str,
        http_user_agent: &str,
        open_ai_key: &str,
        open_ai_model: &str,
        llm_preamble: &str,
    ) -> Result<Self, Error> {
        let journal_mode = SqliteJournalMode::from_str(db_journal_mode)?;

        let db_options = SqliteConnectOptions::from_str(db_options)?.journal_mode(journal_mode);

        let sqlite_pool = SqlitePool::connect_with(db_options).await?;

        let http_client = Client::builder()
            .user_agent(http_user_agent)
            .connect_timeout(Duration::from_secs(5))
            .build()?;

        let open_ai_client = openai::Client::new(open_ai_key);
        let open_ai_agent = open_ai_client
            .agent(open_ai_model)
            .preamble(llm_preamble)
            .build();

        Ok(Self {
            open_ai_client,
            open_ai_agent,
            sqlite_pool,
            http_client,
        })
    }
}
```

This constructor now handles *five* separate input parameters, each contributing to the configuration of different parts of the application. It creates a `SqliteConnectOptions` object to configure the database with the specified journal mode. The HTTP client is set up with a custom user agent and a longer timeout. The AI client is initialized using an explicit API key, and the agent is constructed with a custom model and preamble.

While none of these steps are especially difficult on their own, the function is starting to grow in complexity. It’s also becoming more **fragile**, as all responsibilities are bundled into one place. Every change to a single subsystem — whether it’s database, HTTP, or AI — requires editing the same constructor.

## Why Modular Constructor Matters

As we've seen in the previous example, even modest configurability can cause a constructor's complexity to grow rapidly. With just a few additional fields or customization options, the function becomes harder to maintain, test, and reason about.

In many cases, there's no single “correct” way to construct an application context. For example, you might want to retain both versions of the `new` constructor from earlier: a minimal one for unit tests with default values, and a more elaborate, configurable one for production. In fact, it's common for different parts of an application to require different levels of configurability—some using defaults, others requiring fine-grained setup.

To manage this complexity, Rust developers often reach for the [*builder pattern*](https://rust-unofficial.github.io/patterns/patterns/creational/builder.html). This involves creating a separate builder struct, typically with optional or defaultable fields and fluent setter methods. The builder is used to gradually assemble values before producing the final struct.

## Challenges for Modular Builders

The traditional builder pattern works, but it comes with serious limitations — especially when extensibility and modularity are important.

The first limitation is **tight coupling**. A builder is usually tied directly to a specific target struct. If you create a new context that’s only slightly different from an existing one, you often have to duplicate the entire builder implementation, even if most of the logic is the same.

Second, builders are typically **non-extensible**. If you want to extend the construction logic — say, by adding a new step to initialize an additional field — you usually have to modify the original builder struct. This makes it hard to share construction logic across crates or teams without exposing internal implementation details.

The root cause of these problems is that struct construction in Rust typically requires direct access to the **concrete type**. That means the builder must know the exact shape of the final struct and have access to all its field values up front. If you need intermediate values or want to plug in custom build steps, those values must be manually threaded through the builder and its state.

This rigidity makes it difficult to define reusable, composable building blocks—especially in large or evolving codebases.

## Modular Builders with CGP

Earlier versions of CGP also ran into these limitations. When writing *context-generic* code, we wanted to construct structs in a way that didn’t require knowing their concrete types ahead of time. But because Rust structs require all field values to be present simultaneously at construction time, we couldn’t easily implement flexible or reusable context-generic constructors.

With the latest release, that limitation is fully resolved.

CGP now supports **modular, extensible struct builders** that can be composed from smaller, independent parts. Each module can define how to build a piece of a context struct, and the builder automatically merges them—without needing to know the final shape of the struct ahead of time.

This opens the door to a new style of constructor logic: one that is **modular**, **composable**, and **context-generic**. You can define builders for individual subsystems (e.g., database, HTTP client, AI agent), and combine them to build any compatible application context.

In the next section, we’ll revisit the constructor examples we’ve already seen — and show how to rewrite them using CGP’s new builder pattern to achieve clean, modular, and reusable construction logic.
## A Modular SQLite Builder

Let’s now explore how to implement modular construction of the `App` context using multiple CGP providers. We’ll start by defining a default SQLite builder provider using CGP's `Handler` component:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildDefaultSqliteClient
where
    Build: HasSqlitePath + CanRaiseAsyncError<sqlx::Error>,
{
    type Output = SqliteClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let sqlite_pool = SqlitePool::connect(build.db_path())
            .await
            .map_err(Build::raise_error)?;

        Ok(SqliteClient { sqlite_pool })
    }
}
```

In this example, we define `BuildDefaultSqliteClient` as a CGP provider that implements the `Handler` component. This is the same `Handler` trait we introduced in [Hypershell](/blog/hypershell-release/#handler-component), where it was used to power shell-like pipelines. Here, we repurpose the same trait to construct modular context components. This demonstrates how general-purpose the `Handler` trait is — it can be used for pipelines, API endpoints, matchers, and now, context builders.

The `Build` type parameter refers to a generic **builder context**, not the final `App` struct. This context includes the inputs required to construct a `SqliteClient`. In this case, the builder must be able to provide a database path, as well as a way to raise errors from `sqlx`. These requirements are expressed through the `HasSqlitePath` and `CanRaiseAsyncError` constraints.

The `HasSqlitePath` trait is defined as follows:

```rust
#[cgp_auto_getter]
pub trait HasSqlitePath {
    fn db_path(&self) -> &str;
}
```

By marking the trait with [`#[cgp_auto_getter]`](https://patterns.contextgeneric.dev/generic-accessor-providers.html#the-cgp_auto_getter-macro), CGP can automatically implement this trait for any builder context that contains a `db_path` field of type `String`. This automatic implementation reduces boilerplate and ensures that any context with the appropriate fields can satisfy the trait bounds.

Although our example does not make use of the `Code` or `Input` parameters, they remain part of the `Handler` signature. The `Code` parameter may be used for *compile-time options* that allow contexts to be constructed in multiple ways. Meanwhile, `Input` typically refers to the **partial** value of the final struct being built. These capabilities are useful in more advanced scenarios, but we will leave their explanation for a later section.

In this implementation, the `handle` method simply connects to the SQLite database using the provided path, wraps the resulting pool in a `SqliteClient` struct, and returns it. The `SqliteClient` is defined as:

```rust
#[derive(HasField, HasFields, BuildField)]
pub struct SqliteClient {
    pub sqlite_pool: SqlitePool,
}
```

This struct acts as a wrapper around `SqlitePool` and serves as the output of our modular builder. Although `BuildDefaultSqliteClient` does not build the full `App` context, we can merge its output into `App` using CGP’s `build_from` mechanism we covered earlier. Deriving `HasField`, `HasFields`, and `BuildField` on `SqliteClient` allows it to be safely and automatically merged into the final context during composition.

At this point, you might be wondering why so much infrastructure is needed just to call `SqlitePool::connect`. The answer is that, while this example is simple, real-world construction logic can be much more complex. By encapsulating each part of the logic into modular components, we gain flexibility, reusability, and testability.

To demonstrate this flexibility, consider a more complex version of the SQLite builder. This version uses connection options and journal mode configuration rather than a simple path string:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildSqliteClient
where
    Build: HasSqliteOptions + CanRaiseAsyncError<sqlx::Error>,
{
    type Output = SqliteClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let journal_mode =
            SqliteJournalMode::from_str(build.db_journal_mode()).map_err(Build::raise_error)?;

        let db_options = SqliteConnectOptions::from_str(build.db_options())
            .map_err(Build::raise_error)?
            .journal_mode(journal_mode);

        let sqlite_pool = SqlitePool::connect_with(db_options)
            .await
            .map_err(Build::raise_error)?;

        Ok(SqliteClient { sqlite_pool })
    }
}

#[cgp_auto_getter]
pub trait HasSqliteOptions {
    fn db_options(&self) -> &str;

    fn db_journal_mode(&self) -> &str;
}
```

In this version, `BuildSqliteClient` constructs a `SqliteClient` using fully configurable connection options. The `Build` context must now implement `HasSqliteOptions`, a trait that provides both the connection URI and the desired journal mode.

This example illustrates the key advantage of modular builders: the builder logic is entirely **decoupled** from the context itself. If we want to use `BuildDefaultSqliteClient`, we can define a simple builder context with just a `db_path` field. If we switch to `BuildSqliteClient`, we only need to provide a different context that includes `db_options` and `db_journal_mode`. All other components of the builder can remain unchanged.

Thanks to this decoupling, we can easily swap in different builder providers depending on the needs of the environment — be it development, testing, or production — without rewriting the entire construction logic. This modularity makes CGP builders highly scalable and adaptable to real-world applications.

## HTTP Client Builder

Just as we modularized the construction of the SQLite client, we can also define a modular builder for an HTTP client using CGP. In this case, we will construct a custom `reqwest` client with specific configuration options. To keep the focus on advanced use cases, we will skip the simpler version and go directly to the more complex construction logic.

The HTTP client builder is implemented as follows:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildHttpClient
where
    Build: HasHttpClientConfig + CanRaiseAsyncError<reqwest::Error>,
{
    type Output = HttpClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let http_client = Client::builder()
            .user_agent(build.http_user_agent())
            .connect_timeout(Duration::from_secs(5))
            .build()
            .map_err(Build::raise_error)?;

        Ok(HttpClient { http_client })
    }
}
```

This provider, `BuildHttpClient`, is structured very similarly to `BuildSqliteClient`. It implements the `Handler` trait and defines `HttpClient` as its output. The `Build` context is required to implement two traits: `HasHttpClientConfig`, which supplies the necessary configuration values, and `CanRaiseAsyncError<reqwest::Error>`, which allows the context to convert `reqwest` errors into its own error type.

The required configuration is minimal. In this case, we only need a user agent string, which is defined through the following trait:

```rust
#[cgp_auto_getter]
pub trait HasHttpClientConfig {
    fn http_user_agent(&self) -> &str;
}
```

As with the previous examples, the `#[cgp_auto_getter]` macro ensures that this trait is automatically implemented for any context that includes a `http_user_agent` field. This allows us to easily reuse the same trait across multiple builder contexts without having to write additional code.

The output of this builder is a simple wrapper around `reqwest::Client`:

```rust
#[derive(HasField, HasFields, BuildField)]
pub struct HttpClient {
    pub http_client: Client,
}
```

Here again, we derive `HasField`, `HasFields`, and `BuildField` to support field merging into the final context later on. This makes the `HttpClient` output compatible with CGP’s `build_from` mechanism, allowing it to be composed with other builder outputs.

The `handle` method creates a new `reqwest::Client` using the client builder from `reqwest`. It sets the user agent using a value from the context, and specifies a connection timeout of five seconds. The constructed client is then wrapped in the `HttpClient` struct and returned.

Although this example remains relatively simple, it illustrates how each field or component in a context can be modularly constructed using dedicated builder logic. Each builder is independently defined, type-safe, and reusable. If the way we configure our HTTP client changes — for example, if we want to support proxies or TLS settings — we can define a new provider that implements a different construction strategy, without needing to change any of the other components in our application context.

This modularity provides a scalable, maintainable way to construct rich application contexts from independently developed and composable building blocks.

## Combined SQLite and HTTP Client Builder

Before we move on, it is important to emphasize that CGP does **not** require you to break down the construction logic of every component in your application context into separate builders. While the modular approach can offer more flexibility and reuse, you are entirely free to combine multiple construction tasks into a single provider if that better suits your needs.

For example, here is how you might implement a single builder that constructs *both* the SQLite client and the HTTP client together:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildDefaultSqliteAndHttpClient
where
    Build: HasSqlitePath + CanRaiseAsyncError<sqlx::Error>,
{
    type Output = SqliteAndHttpClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let sqlite_pool = SqlitePool::connect(build.db_path())
            .await
            .map_err(Build::raise_error)?;

        let http_client = Client::new();

        Ok(SqliteAndHttpClient { sqlite_pool, http_client })
    }
}

#[derive(HasField, HasFields, BuildField)]
pub struct SqliteAndHttpClient {
    pub sqlite_pool: SqlitePool,
    pub http_client: Client,
}
```

In this implementation, we define a single provider `BuildDefaultSqliteAndHttpClient` that returns a combined struct `SqliteAndHttpClient`, which contains both a `SqlitePool` and a `reqwest::Client`. The construction logic is written in one place, which can be convenient when these components are always used together or when their configuration is tightly integrated.

However, the tradeoff of this approach is that it reduces flexibility. This tight coupling can limit reuse and make future changes more difficult.

That said, the choice of whether to combine or separate builders is **entirely up to you**. CGP does **not** impose any rules on how you must structure your builder logic. It provides the tools to compose and reuse components where helpful, but it leaves design decisions to the developer.

For the remainder of this article, we will continue to use the fully modular approach, breaking construction logic down into smaller, independent units. Our goal is to illustrate the full extent of flexibility and reusability that CGP enables. However, if you prefer a different organizational structure, you are free to structure your builders in whatever way best suits your project.

## ChatGPT Client Builder

Regardless of whether you prefer to split or combine the construction of components such as the SQLite and HTTP clients, there are many situations where it makes sense to separate construction logic into smaller, more focused units. For instance, you might want to offer two versions of your application — one standard version and one "smart" version that includes AI capabilities. In such cases, it is useful to define a separate builder provider for the ChatGPT client, so that AI-related logic can be included only when necessary.

The implementation for the ChatGPT client builder follows the same general pattern as the previous builders. It is defined as follows:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildOpenAiClient
where
    Build: HasOpenAiConfig + HasAsyncErrorType,
{
    type Output = OpenAiClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let open_ai_client = openai::Client::new(build.open_ai_key());
        let open_ai_agent = open_ai_client
            .agent(build.open_ai_model())
            .preamble(build.llm_preamble())
            .build();

        Ok(OpenAiClient {
            open_ai_client,
            open_ai_agent,
        })
    }
}
```

This builder requires the `Build` context to provide three string fields: the OpenAI API key, the model name, and a custom preamble string. These requirements are captured by the `HasOpenAiConfig` trait:

```rust
#[cgp_auto_getter]
pub trait HasOpenAiConfig {
    fn open_ai_key(&self) -> &str;

    fn open_ai_model(&self) -> &str;

    fn llm_preamble(&self) -> &str;
}
```

As with the other providers, we use the `#[cgp_auto_getter]` macro to automatically implement the trait, as long as the builder context contains the corresponding fields and derives `HasField`.

The `BuildOpenAiClient` provider returns an `OpenAiClient` struct that wraps two values: the low-level `openai::Client` and the higher-level `Agent` configured with the specified model and preamble.

```rust
#[derive(HasField, HasFields, BuildField)]
pub struct OpenAiClient {
    pub open_ai_client: openai::Client,
    pub open_ai_agent: Agent<openai::CompletionModel>,
}
```

By defining this logic in a standalone builder provider, we can easily opt in or out of ChatGPT support in our application context. This modular approach keeps the design flexible and makes it easier to extend or modify individual components without disrupting the overall system.

## Builder Context

Now that we have implemented builder providers for SQLite, HTTP, and ChatGPT clients, we can demonstrate how to combine them in a complete builder context that constructs the final `App` instance. Defining this context is surprisingly concise and requires only a few lines of code:

```rust
#[cgp_context]
#[derive(HasField, Deserialize)]
pub struct FullAppBuilder {
    pub db_options: String,
    pub db_journal_mode: String,
    pub http_user_agent: String,
    pub open_ai_key: String,
    pub open_ai_model: String,
    pub llm_preamble: String,
}
```

Here, we define a `FullAppBuilder` struct that includes all of the fields required by the three individual builder providers. The `#[cgp_context]` macro enables the CGP capabilities for the context struct, while the `HasField` derive macro enables automatic implementation of the necessary accessor traits using `#[cgp_auto_getter]`. In addition, we derive `Deserialize` so that `FullAppBuilder` can be easily loaded from a configuration file in formats such as JSON or TOML.

Next, we wire up the builder context using the `delegate_components!` macro:

```rust
delegate_components! {
    FullAppBuilderComponents {
        ErrorTypeProviderComponent:
            UseAnyhowError,
        ErrorRaiserComponent:
            RaiseAnyhowError,
        HandlerComponent:
            BuildAndMergeOutputs<
                App,
                Product![
                    BuildSqliteClient,
                    BuildHttpClient,
                    BuildOpenAiClient,
                ]>,
    }
}
```

This macro allows us to delegate the implementation of various components of the builder context. First, we configure error handling by using the `cgp-anyhow-error` library. The `UseAnyhowError` provider specifies that our abstract `Error` type will be instantiated to `anyhow::Error`, and the `RaiseAnyhowError` provider allows conversion from error types implementing `core::error::Error`, like `sqlx::Error` and `reqwest::Error`, into `anyhow::Error`.

## Builder Dispatcher

In the example above, we used a special **builder dispatcher** called `BuildAndMergeOutputs` to implement the `HandlerComponent`. This dispatcher allows us to construct the final `App` type by sequentially combining the outputs of multiple builder providers. We specify the target `App` type as the output of the build process, and then pass in a _type-level list_ of builder providers using the `Product!` macro. In this case, we used `BuildSqliteClient`, `BuildHttpClient`, and `BuildOpenAiClient`, all of which we implemented previously.

To understand how `BuildAndMergeOutputs` operates under the hood, let us walk through a manual implementation that performs the same task:

```rust
#[cgp_new_provider]
impl<Code: Send, Input: Send> Handler<FullAppBuilder, Code, Input> for BuildApp {
    type Output = App;

    async fn handle(context: &FullAppBuilder, code: PhantomData<Code>, _input: Input) -> Result<App, Error> {
        let app = App::builder()
            .build_from(BuildSqliteClient::handle(context, code, ()).await?)
            .build_from(BuildHttpClient::handle(context, code, ()).await?)
            .build_from(BuildOpenAiClient::handle(context, code, ()).await?)
            .finalize_build();

        Ok(app)
    }
}
```

This manual implementation demonstrates the boilerplate that would be necessary if we did not use `BuildAndMergeOutputs`. Here, we define `BuildApp` as a _context-specific_ provider for the `FullAppBuilder` context. It implements the `Handler` trait for any `Code` and `Input` types.

Within the `handle` method, we construct the `App` in a step-by-step manner, similar to how we built complex types earlier in the [safe struct building](#safe-struct-building) section. We begin by initializing an empty builder with `App::builder()`. Next, we invoke the `handle` method on each of the individual providers — `BuildSqliteClient`, `BuildHttpClient`, and `BuildOpenAiClient` — passing them the shared context and `PhantomData` for the code. The resulting outputs are incrementally merged into the builder using `build_from`, and finally, `finalize_build` is called to produce the completed `App` instance.

In this example, we ignore the original `Input` parameter and instead pass `()` to each sub-handler for simplicity. In the actual implementation of `BuildAndMergeOutputs`, a *reference* to the intermediate builder is instead passed along as input to each sub-handler to support more advanced use cases. However, we have omitted that detail here to focus on the overall structure.

While the manual implementation of `BuildApp` is relatively easy to follow, it is also quite repetitive. The main benefit of `BuildAndMergeOutputs` is that it eliminates this boilerplate by abstracting away the repetitive logic of chaining multiple builder steps and threading intermediary results. Furthermore, `BuildAndMergeOutputs` is implemented with the necessary generic parameters and constraints to work with _any_ context type, as compared to being tied to the `App` context that we defined.

Aside from this reduction in verbosity, the behavior remains conceptually the same as what is shown in the manual example.

## Building the App

With the builder context defined, we can now construct the full `App` by simply instantiating the builder and calling its `handle` method:

```rust
async fn main() -> Result<(), Error> {
    let builder = FullAppBuilder {
        db_options: "file:./db.sqlite".to_owned(),
        db_journal_mode: "WAL".to_owned(),
        http_user_agent: "SUPER_AI_AGENT".to_owned(),
        open_ai_key: "1234567890".to_owned(),
        open_ai_model: "gpt-4o".to_owned(),
        llm_preamble: "You are a helpful assistant".to_owned(),
    };

    let app = builder.handle(PhantomData::<()>, ()).await?;

    /* Call methods on the app here */

    Ok(())
}
```

In this example, we initialize `FullAppBuilder` by filling in the required configuration values. We then call `builder.handle()` to construct the `App`. The `handle` method requires two arguments: a `Code` type and an `Input` value. However, because neither of these are constrained in any way in our example, we can simply pass *any* type we want, such as the unit type `()` for both. This simplifies to the equivalent of calling `builder.handle()` with no argument in practice.

This example illustrates how CGP allows new builder contexts to be defined with minimal effort by composing multiple independent builder providers—none of which require knowledge of the final type being constructed.

Rather than writing custom constructor functions that take numerous arguments, we define a builder struct where each required input becomes a field. Instead of manually constructing each component of the context, we use `delegate_components!` to connect the appropriate builder providers, which handle the construction logic for us.

By embracing this modular builder approach, our code becomes not only more extensible, but also easier to read, test, and maintain.

## More Builder Examples

At this point, some readers may still be skeptical about the value of modularity offered by CGP builders. Since we’ve only shown a **single** application context and **one** corresponding builder context so far, it might not be obvious why we couldn’t just use a simple `new` constructor function like the one defined at the beginning.

To truly demonstrate the power of modular builders, it’s helpful to explore how CGP makes it easy to define **multiple** contexts that are similar but have slight differences. However, if you're an **advanced reader** already familiar with the benefits of modular design, feel free to [**skip ahead**](#extensible-variants) to continue reading the main content.

### Default Builder

Earlier, we introduced default builders like `BuildDefaultSqliteClient`, which can construct an `App` with default configuration values. These defaults can be combined to define a minimal builder for `App`:

```rust
#[cgp_context]
#[derive(HasField, Deserialize)]
pub struct DefaultAppBuilder {
    pub db_path: String,
}

delegate_components! {
    DefaultAppBuilderComponents {
        ...
        HandlerComponent:
            BuildAndMergeOutputs<
                App,
                Product![
                    BuildDefaultSqliteClient,
                    BuildDefaultHttpClient,
                    BuildDefaultOpenAiClient,
                ]>,
    }
}
```

In this context, the only required configuration is the `db_path`, simplifying the process of constructing an `App`, especially for use cases like unit testing or demos.

### Postgres App

Now suppose we want an enterprise version of the app that uses **Postgres** instead of SQLite. We can define a new `App` context that swaps in `PgPool`:

```rust
#[cgp_context]
#[derive(HasField, HasFields, BuildField)]
pub struct App {
    pub postgres_pool: PgPool,
    pub http_client: Client,
    pub open_ai_client: openai::Client,
    pub open_ai_agent: Agent<openai::CompletionModel>,
}
```

Since the HTTP and ChatGPT logic remains unchanged, we only need to implement a new builder for Postgres:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildPostgresClient
where
    Build: HasPostgresUrl + CanRaiseAsyncError<sqlx::Error>,
{
    type Output = PostgresClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let postgres_pool = PgPool::connect(build.postgres_url())
            .await
            .map_err(Build::raise_error)?;

        Ok(PostgresClient { postgres_pool })
    }
}

#[cgp_auto_getter]
pub trait HasPostgresUrl {
    fn postgres_url(&self) -> &str;
}

#[derive(HasField, HasFields, BuildField)]
pub struct PostgresClient {
    pub postgres_pool: PgPool,
}
```

This builder closely mirrors the SQLite version, but reads the `postgres_url` field from the context instead.

Next, we define a new builder context that includes Postgres configuration:

```rust
#[cgp_context]
#[derive(HasField, Deserialize)]
pub struct AppBuilder {
    pub postgres_url: String,
    pub http_user_agent: String,
    pub open_ai_key: String,
    pub open_ai_model: String,
    pub llm_preamble: String,
}

delegate_components! {
    AppBuilderComponents {
        ...
        HandlerComponent:
            BuildAndMergeOutputs<
                App,
                Product![
                    BuildPostgresClient,
                    BuildHttpClient,
                    BuildOpenAiClient,
                ]>,
    }
}
```

Here, we simply swap in `BuildPostgresClient` instead of `BuildSqliteClient`, while reusing the other builder providers unchanged.

This example highlights a key advantage of CGP over traditional **feature flags**: with CGP, multiple application variants (e.g., SQLite or Postgres) can **coexist** in the same codebase and even be compiled together. In contrast, feature flags often force a binary either/or split at compile time.

By enabling different configurations to exist side-by-side, CGP improves testability and reduces the likelihood of missing edge cases caused by untested feature combinations.

### Anthropic App

Just as we swapped SQLite for Postgres earlier, we can also substitute the AI model used in the application—such as replacing ChatGPT with **Claude**. With CGP, this becomes straightforward: we simply define a new `AnthropicApp` that uses the Anthropic client and agent:

```rust
#[cgp_context]
#[derive(HasField, HasFields, BuildField)]
pub struct AnthropicApp {
    pub sqlite_pool: SqlitePool,
    pub http_client: Client,
    pub anthropic_client: anthropic::Client,
    pub anthropic_agent: Agent<anthropic::completion::CompletionModel>,
}
```

Next, we implement a builder provider to construct the Claude client:

```rust
#[cgp_new_provider]
impl<Build, Code: Send, Input: Send> Handler<Build, Code, Input> for BuildDefaultAnthropicClient
where
    Build: HasAnthropicConfig + HasAsyncErrorType,
{
    type Output = AnthropicClient;

    async fn handle(
        build: &Build,
        _code: PhantomData<Code>,
        _input: Input,
    ) -> Result<Self::Output, Build::Error> {
        let anthropic_client = ClientBuilder::new(build.anthropic_key())
            .anthropic_version(ANTHROPIC_VERSION_LATEST)
            .build();

        let anthropic_agent = anthropic_client
            .agent(anthropic::CLAUDE_3_7_SONNET)
            .preamble(build.llm_preamble())
            .build();

        Ok(AnthropicClient {
            anthropic_client,
            anthropic_agent,
        })
    }
}

#[cgp_auto_getter]
pub trait HasAnthropicConfig {
    fn anthropic_key(&self) -> &str;
    fn llm_preamble(&self) -> &str;
}

#[derive(HasField, HasFields, BuildField)]
pub struct AnthropicClient {
    pub anthropic_client: anthropic::Client,
    pub anthropic_agent: Agent<CompletionModel>,
}
```

With the builder provider in place, we define a new builder context that includes the Anthropic API key and wire it up using `BuildDefaultAnthropicClient`:

```rust
#[cgp_context]
#[derive(HasField, Deserialize)]
pub struct AppBuilder {
    pub db_options: String,
    pub db_journal_mode: String,
    pub http_user_agent: String,
    pub anthropic_key: String,
    pub llm_preamble: String,
}

delegate_components! {
    AppBuilderComponents {
        ...
        HandlerComponent:
            BuildAndMergeOutputs<
                AnthropicApp,
                Product![
                    BuildSqliteClient,
                    BuildHttpClient,
                    BuildDefaultAnthropicClient,
                ]>,
    }
}
```

This example shows how effortlessly CGP supports variation and customization. The same modular pattern can be reused to swap in different components—databases, clients, or agents—without rewriting core application logic.

In fact, the process becomes so systematic that it’s easy to imagine an AI tool like **Claude Code** automating the entire setup given the right prompt and documentation.

### Anthropic and ChatGPT Builder

It’s impressive that CGP lets us easily swap ChatGPT for Claude. But what’s even better is that we don’t have to choose at all—we can include **both** AI agents in the same application.

This could be useful for scenarios where combining the strengths of multiple models improves the overall intelligence or reliability of your application. More importantly, it demonstrates that CGP is not just about selecting one provider over another — it’s also about composing multiple providers together in a clean, modular way.

We begin by defining an `AnthropicAndChatGptApp` context that includes both Claude and ChatGPT clients:

```rust
#[cgp_context]
#[derive(HasField, HasFields, BuildField)]
pub struct AnthropicAndChatGptApp {
    pub sqlite_pool: SqlitePool,
    pub http_client: Client,
    pub anthropic_client: anthropic::Client,
    pub anthropic_agent: Agent<anthropic::completion::CompletionModel>,
    pub open_ai_client: openai::Client,
    pub open_ai_agent: Agent<openai::CompletionModel>,
}
```

Next, we define a builder context that includes configuration fields for both AI platforms:

```rust
#[cgp_context]
#[derive(HasField, Deserialize)]
pub struct AnthropicAndChatGptAppBuilder {
    pub db_options: String,
    pub db_journal_mode: String,
    pub http_user_agent: String,
    pub anthropic_key: String,
    pub open_ai_key: String,
    pub open_ai_model: String,
    pub llm_preamble: String,
}
```

In the component wiring, we include both `BuildDefaultAnthropicClient` and `BuildOpenAiClient` in the provider list:

```rust
delegate_components! {
    AnthropicAndChatGptAppBuilderComponents {
        ...
        HandlerComponent:
            BuildAndMergeOutputs<
                AnthropicAndChatGptApp,
                Product![
                    BuildSqliteClient,
                    BuildHttpClient,
                    BuildDefaultAnthropicClient,
                    BuildOpenAiClient,
                ]>,
    }
}
```

With just a few extra lines, we’ve created a dual-agent AI app that can leverage both Claude and ChatGPT simultaneously.

It’s also worth noting that the `llm_preamble` field is reused by both the Claude and ChatGPT builders. This demonstrates CGP’s flexibility in sharing input values across multiple providers—without requiring any manual coordination or boilerplate.

This kind of seamless reuse and composition is where CGP truly shines: giving you fine-grained control over how your application is assembled, while keeping your code modular and maintainable.

### Multi-Context Builder

Looking closely at the `AnthropicAndChatGptAppBuilder` that we previously defined, we can observe that it already includes all the necessary fields required to construct the Claude-only and ChatGPT-only applications as well. This means we can reuse the same builder to construct **all three** versions of our application contexts, simply by changing how the builder is wired.

To achieve this, we take advantage of the `Code` type parameter, which allows us to emulate DSL-like behavior similar to what is seen in [Hypershell](/blog/hypershell-release/#abstract-syntax). We begin by defining distinct marker types that represent the different build modes:

```rust
pub struct BuildChatGptApp;
pub struct BuildAnthropicApp;
pub struct BuildAnthropicAndChatGptApp;
```

Using these types, we can apply the [`UseDelegate`](/blog/hypershell-release/#generic-dispatcher) pattern to route the `Handler` implementation to different builder pipelines depending on the code passed in. This enables conditional wiring based on the selected application mode:

```rust
delegate_components! {
    AnthropicAndChatGptAppBuilderComponents {
        ...
        HandlerComponent:
            UseDelegate<new BuilderHandlers {
                BuildAnthropicAndChatGptApp:
                    BuildAndMergeOutputs<
                        AnthropicAndChatGptApp,
                        Product![
                            BuildSqliteClient,
                            BuildHttpClient,
                            BuildDefaultAnthropicClient,
                            BuildOpenAiClient,
                        ]>,
                BuildChatGptApp:
                    BuildAndMergeOutputs<
                        ChatGptApp,
                        Product![
                            BuildSqliteClient,
                            BuildHttpClient,
                            BuildOpenAiClient,
                        ]>,
                BuildAnthropicApp:
                    BuildAndMergeOutputs<
                        AnthropicApp,
                        Product![
                            BuildSqliteClient,
                            BuildHttpClient,
                            BuildDefaultAnthropicClient,
                        ]>,
            }>,
    }
}
```

Now, when we want to construct a specific application context, we only need to change the `Code` type by using `PhantomData`. This gives us a flexible, type-safe way to select the desired builder pipeline at runtime:

```rust
pub async fn main() -> Result<(), Error> {
    let builder = AnthropicAndChatGptAppBuilder {
        db_options: "file:./db.sqlite".to_owned(),
        db_journal_mode: "WAL".to_owned(),
        http_user_agent: "SUPER_AI_AGENT".to_owned(),
        anthropic_key: "1234567890".to_owned(),
        open_ai_key: "1234567890".to_owned(),
        open_ai_model: "gpt-4o".to_owned(),
        llm_preamble: "You are a helpful assistant".to_owned(),
    };

    let chat_gpt_app: ChatGptApp =
        builder.handle(PhantomData::<BuildChatGptApp>, ()).await?;

    let anthropic_app: AnthropicApp =
        builder.handle(PhantomData::<BuildAnthropicApp>, ()).await?;

    let combined_app: AnthropicAndChatGptApp =
        builder.handle(PhantomData::<BuildAnthropicAndChatGptApp>, ()).await?;

    /* Use the application contexts here */

    Ok(())
}
```

This example highlights how CGP's DSL features are not limited to building full-fledged domain-specific languages like [Hypershell](/blog/hypershell-release/). Even in this lightweight form, they are immensely valuable for **labeling and routing** different behaviors based on combinations of builder providers.

In essence, we are still constructing a mini-DSL, albeit one composed of simple symbolic "statements" without complex language constructs. This approach not only brings expressive power to your builder logic, but also lays the groundwork for future extensions—such as richer abstract syntaxes—using the same techniques introduced by Hypershell.

# Extensible Variants

Earlier, we explored how CGP uses the extensible builder pattern to enable modular construction of context structs. In this section, we will see how a similar approach can be applied to context **enums**, allowing each variant to be destructured and handled by a flexible, composable set of handlers.

In Rust and many object-oriented languages, this pattern is commonly referred to as the [**visitor pattern**](https://rust-unofficial.github.io/patterns/patterns/behavioural/visitor.html). However, Rust’s powerful `enum` and `match` features often reduce the need for the visitor pattern, especially when the concrete enum type is known. In such cases, developers can simply use `match` expressions to handle each variant explicitly and concisely.

Despite this, the visitor pattern remains useful in situations where the concrete enum type is **unknown** or abstracted away. This is especially true in libraries like [`serde`](https://docs.rs/serde/latest/serde/de/trait.Visitor.html) and [`syn`](https://docs.rs/syn/latest/syn/visit/trait.Visit.html), where visitors are used to traverse abstract syntax trees or serialization payloads without tying the implementation to a specific format or structure. For instance, in `serde`, deserialization is driven by a visitor provided by the target type, which walks through structures like JSON or TOML without coupling the deserializer to any specific data format.

## Limitations of the Traditional Visitor Pattern

While the visitor pattern is useful, it suffers from a major drawback: it is inherently **closed for extension**. All possible variants and visitable types must be declared upfront in the visitor interface, and it is challenging to add or remove variants later without breaking existing implementations.

For example, consider the `Visitor` trait in `serde`, which defines methods for visiting a fixed set of primitive types — up to 128-bit integers. If a developer wants to deserialize a type that contains a [`U256`](https://docs.rs/primitive-types/latest/primitive_types/struct.U256.html) value, there is no way to extend the `Visitor` trait to support native 256-bit integers. Likewise, if someone builds a new serialization format that introduces support for such a type, it cannot cleanly integrate with `serde` because the trait cannot be expanded.

To work around this, `serde` includes a broad set of 26 visitor methods in its core `Visitor` trait to accommodate a wide range of cases. However, this introduces the opposite problem: when a serialization format **does not** support a specific visitor method, the only option is to return a **runtime error**. There is no way to signal at compile time that a type is incompatible with the format, even if it formally implements `Serialize` or `Deserialize`.

This mismatch becomes especially noticeable when using compact formats like [`postcard`](https://docs.rs/postcard) or [`bincode`](https://docs.rs/bincode), which support only a small subset of types compared to JSON. These libraries accept any type implementing `Deserialize`, but compatibility is only verified at runtime — leaving users to discover format mismatches through runtime errors instead of compile-time errors.

In short, the traditional visitor pattern tends to be either too **restrictive** (by enforcing a closed set of operations) or too **permissive** (by relying on runtime errors to reject unsupported operations). What’s needed is a more flexible, composable alternative — one that allows both sides (visitor and visitee) to express precise requirements at compile time.

This is exactly the problem that the **extensible visitor pattern** in CGP aims to solve. It enables open-ended, modular visitors that preserve **type safety** and **extensibility**, without the pitfalls of runtime errors or rigid interfaces.

## The Expression Problem

While it’s theoretically possible to replace `serde`’s visitor pattern with CGP’s extensible alternative, doing so would require significant refactoring and is outside the scope of this post. Instead, we’ll explore a *simpler* but well-known challenge that illustrates the same limitations: the [**expression problem**](https://en.wikipedia.org/wiki/Expression_problem).

Suppose we want to implement an interpreter for a toy arithmetic language in Rust. This language might support basic math expressions like `1 + (2 * 3)`. A typical way to represent such a language is with an enum like this:

```rust
pub enum MathExpr {
    Literal(u64),
    Plus(Box<MathExpr>, Box<MathExpr>),
    Times(Box<MathExpr>, Box<MathExpr>),
}
```

Here, `MathExpr` represents arithmetic expressions. The `Plus` and `Times` variants contain boxed sub-expressions, and `Literal` holds an integer value. The use of `Box` is necessary due to Rust’s size constraints for recursive data structures.

To evaluate these expressions, we can implement a straightforward `eval` function:

```rust
pub fn eval(expr: MathExpr) -> u64 {
    match expr {
        MathExpr::Literal(value) => value,
        MathExpr::Plus(a, b) => eval(*a) + eval(*b),
        MathExpr::Times(a, b) => eval(*a) * eval(*b),
    }
}
```

This works well for small examples. But real-world interpreters quickly grow in complexity. Each evaluation case might span dozens — or hundreds — of lines of code. Additionally, the enum itself might have many more variants. For example, [`syn::Expr`](https://docs.rs/syn/latest/syn/enum.Expr.html), a real-world expression type for Rust, defines over *40 variants*.

Let’s assume our toy `Expr` is similarly complex. Now imagine that alongside `eval`, we also want to define other operations, like pretty-printing:

```rust
pub fn expr_to_string(expr: &MathExpr) -> String {
    match expr {
        MathExpr::Literal(value) => value.to_string(),
        MathExpr::Plus(a, b) => format!("({} + {})", expr_to_string(a), expr_to_string(b)),
        MathExpr::Times(a, b) => format!("({} * {})", expr_to_string(a), expr_to_string(b)),
    }
}
```

Here lies the crux of the expression problem: as the language evolves, we frequently need to *add* new expression variants or *remove* old ones. But any modification to the `Expr` enum forces us to update *all* pattern-matching functions like `eval`, `expr_to_string`, and others. The enum becomes **tightly coupled** to every function that consumes it.

Worse, this coupling is not easy to break. The recursive nature of `MathExpr` — where variants like `Plus` contain other `MathExpr` values — means even modular helper functions (e.g., `eval_plus`) must still operate on `MathExpr`, perpetuating the tight dependency.

This isn’t a problem unique to interpreters. Many recursive data structures — like JSON `Value` types — suffer from similar issues. A JSON object may contain maps of nested `Value`s, making any function over the type deeply tied to its structure.

Because of this, extending or experimenting with the enum often becomes burdensome. If the type is part of an upstream crate, users may need to submit a **pull request** just to add a variant. And if the maintainer *declines*, downstream users may be forced to **fork** the crate just to gain the flexibility they need.

In the next section, we’ll explore how CGP's extensible visitor pattern addresses this problem — by **decoupling** the implementation of each variant from the concrete enum definition.

## Evaluator Computer

To demonstrate how CGP enables extensible and decoupled evaluation logic, we will now walk through how to implement a small part of the `eval` function — specifically, the logic for handling the `Plus` operator. Rather than tying ourselves to a fixed `Expr` enum, we begin by defining `Plus` as an independent struct:

```rust
pub struct Plus<Expr> {
    pub left: Box<Expr>,
    pub right: Box<Expr>,
}
```

In this definition, `Plus` is no longer a variant of a hardcoded enum. Instead, it is a *generic* data structure that takes an `Expr` type parameter. This parameter represents the broader expression type and allows `Plus` to be reused in many different expression trees. The `left` and `right` operands are wrapped in `Box` to support recursive structures while still satisfying Rust’s size requirements later on.

To actually evaluate such a sub-expression, CGP introduces the concept of a **Computer** — a CGP component designed for pure computation. It is defined as follows:

```rust
#[cgp_component(Computer)]
pub trait CanCompute<Code, Input> {
    type Output;

    fn compute(&self, _code: PhantomData<Code>, input: Input) -> Self::Output;
}
```

This trait behaves similarly to the `Handler` trait introduced earlier, but with one key distinction: `compute` is a *synchronous* function and does not return a `Result`. It is called a *computer* because it embodies a pure, deterministic *computation* that transforms input into output.

The `Computer` trait serves as the foundation for extensible evaluation. It abstracts the idea of computation away from any specific expression type or evaluation strategy. Using this abstraction, we can implement evaluation logic for each sub-expression in isolation. For example, here is how we define a provider for evaluating the `Plus` struct:

```rust
#[cgp_new_provider]
impl<Context, Code, MathExpr, Output> Computer<Context, Code, Plus<MathExpr>> for EvalAdd
where
    Context: CanCompute<Code, MathExpr, Output = Output>,
    Output: Add<Output = Output>,
{
    type Output = Output;

    fn compute(
        context: &Context,
        code: PhantomData<Code>,
        Plus { left, right }: Plus<MathExpr>,
    ) -> Self::Output {
        let output_a = context.compute(code, *left);
        let output_b = context.compute(code, *right);

        output_a + output_b
    }
}
```

This implementation defines `EvalAdd` as a `Computer` with `Plus<MathExpr>` as input. It works generically over any `Context`, `Code`, and `MathExpr` type, as long as the context knows how to compute `MathExpr` and the resulting output type supports the `Add` trait. In other words, the context must be able to evaluate each operand, and the results must be addable.

By using `context.compute(...)` recursively on the left and right operands, we evaluate each sub-expression and then add the results together. This setup allows us to write clean, modular logic that does not assume anything about the shape of the expression tree or the numeric type being used.

The same approach applies to other arithmetic operations. For example, we can implement a provider for multiplication as follows:

```rust
pub struct Times<Expr> {
    pub left: Box<Expr>,
    pub right: Box<Expr>,
}

#[cgp_new_provider]
impl<Context, Code, MathExpr, Output> Computer<Context, Code, Times<MathExpr>> for EvalMultiply
where
    Context: CanCompute<Code, MathExpr, Output = Output>,
    Output: Mul<Output = Output>,
{
    type Output = Output;

    fn compute(
        context: &Context,
        code: PhantomData<Code>,
        Times { left, right }: Times<MathExpr>,
    ) -> Output {
        let output_a = context.compute(code, *left);
        let output_b = context.compute(code, *right);

        output_a * output_b
    }
}
```

Here, we follow the exact same structure. The only difference is that we require the output type to implement `Mul` instead of `Add`, and we use the `*` operator to combine the results.

Finally, we handle literals using the following straightforward implementation:

```rust
pub struct Literal<T>(pub T);

#[cgp_new_provider]
impl<Context, Code, T> Computer<Context, Code, Literal<T>> for EvalLiteral {
    type Output = T;

    fn compute(_context: &Context, _code: PhantomData<Code>, Literal(value): Literal<T>) -> T {
        value
    }
}
```

The `EvalLiteral` provider simply returns the inner value. It doesn’t depend on any context or recursive evaluation, making it the simplest form of a computer.

What’s remarkable about this setup is how each of the providers — `EvalAdd`, `EvalMultiply`, and `EvalLiteral` — is completely **decoupled** from both each other and the concrete expression type. These components can live in separate crates or modules and still be composed together to form a complete evaluator.

This modularity is precisely the power that CGP brings to the table. Instead of forcing every part of your evaluator into a rigid, monolithic structure, you can build each piece independently and combine them later. The result is not only cleaner and more maintainable code, but also an evaluation engine that is fully open for extension — without giving up on compile-time guarantees.

## Evaluating Concrete Expressions

With our evaluation logic defined for individual expression types like `Plus`, `Times`, and `Literal`, the next step is to bring everything together into a fully functional evaluator. To do this, we first define a concrete expression type named `MathExpr`:

```rust
#[derive(Debug, HasFields, FromVariant, ExtractField)]
pub enum MathExpr {
    Plus(Plus<MathExpr>),
    Times(Times<MathExpr>),
    Literal(Literal<u64>),
}
```

Notice that instead of embedding the structure of each expression variant directly inside the enum, we define each variant to wrap one of the standalone structs we previously created. This design is intentional. By keeping each sub-expression — such as `Plus` and `Times` — as its own modular type, we can reuse and compose them in more flexible ways. To complete the recursive structure, we instantiate the generic type parameter `MathExpr` inside each sub-expression, allowing the expression tree to contain arbitrarily nested expressions. For the `Literal` case, we keep things simple by hardcoding the value type to `u64`.

Now that we have our enum, we need to define the context in which evaluation will happen. In CGP, this is done using a `#[cgp_context]` declaration:

```rust
#[cgp_context]
pub struct Interpreter;
```

This `Interpreter` struct will serve as the central context object for our evaluator. In this particular example, we do not require the context to hold any runtime data, so the struct is left empty. Its only purpose is to act as a compile-time container that wires together the correct provider implementations.

The actual wiring is handled through CGP’s powerful delegation system, which allows us to map input types to their corresponding computation logic. Here's how we set it up:

```rust
delegate_components! {
    InterpreterComponents {
        ComputerComponent:
            UseInputDelegate<
                new EvalComponents {
                    Expr: DispatchEval,
                    Plus<MathExpr>: EvalAdd,
                    Times<MathExpr>: EvalMultiply,
                    Literal<u64>: EvalLiteral,
                }
            >,
    }
}
```

In this block, we define the `InterpreterComponents` wiring configuration, which tells CGP how to resolve the `Computer` trait for each expression type. At the heart of this configuration is `UseInputDelegate`, a [**generic dispatcher**](/blog/hypershell-release/#input-based-dispatch) that automatically selects the correct provider based on the input type at compile time.

This dispatcher operates over the inner `EvalComponents` table, which we create on the fly using the `new` keyword. This `EvalComponents` struct maps each expression type to its associated computation provider:

* `MathExpr` is mapped to `DispatchEval`, which acts as a dispatcher that dispatches to one of the sub-expression types based on the variant.
* `Plus<MathExpr>` is evaluated using `EvalAdd`.
* `Times<MathExpr>` is evaluated using `EvalMultiply`.
* `Literal<u64>` is handled by `EvalLiteral`.

Because `UseInputDelegate` operates at the type level, the entire dispatch process is fully type-safe and resolved at compile time. There is no need for `match` statements, no runtime type checks, and no boilerplate glue code. The trait system simply composes itself from the parts we’ve defined.

## Dispatching Eval

With the components for evaluating individual sub-expressions in place, we now turn our attention to the final piece of the puzzle: evaluating the main `Expr` enum itself. To accomplish this, we delegate the `MathExpr` type to a special provider named `DispatchEval`, which is defined alongside the `Interpreter` context like so:

```rust
#[cgp_new_provider]
impl<Code> Computer<Interpreter, Code, MathExpr> for DispatchEval {
    type Output = u64;

    fn compute(context: &Interpreter, code: PhantomData<Code>, expr: MathExpr) -> Self::Output {
        <MatchWithValueHandlers>::compute(context, code, expr)
    }
}
```

Here, `DispatchEval` is implemented as a *context-specific* provider. That means it only applies when we are evaluating expressions in the `Interpreter` context, and it handles the concrete `Expr` enum as input. Rather than directly writing out how each variant of the enum is evaluated, we delegate that responsibility to a special **visitor dispatcher** called `MatchWithValueHandlers`.

This dispatcher is one of the key tools provided by CGP. It automatically maps each enum variant to the appropriate computation provider we registered earlier in `EvalComponents`. In effect, `MatchWithValueHandlers` performs dispatch on the matching of variants at compile time. The implementation of `DispatchEval` is simply a wrapper around this dispatcher, but that wrapper plays a crucial role.

So why do we need this wrapper in the first place? It comes down to a subtle limitation in Rust’s trait resolution system. If we try to directly wire the `Computer` handler for `Expr` to `MatchWithValueHandlers`, the compiler runs into a cyclic dependency: to implement the trait, it needs to evaluate the variant-specific providers like `EvalAdd`, which themselves rely on `MatchWithValueHandlers`. The result is a cryptic “overflowing requirements” error.

By inserting this wrapper layer with `DispatchEval`, we sidestep that issue. Rust is able to mark the trait as implemented before diving into the body of the method, effectively breaking the cycle. It’s a well-known trick in the world of advanced Rust trait design, and CGP uses it elegantly to keep everything modular and composable.

To understand what `MatchWithValueHandlers` is doing under the hood, imagine manually writing out the dispatch logic like this:

```rust
#[cgp_new_provider]
impl<Code> Computer<Interpreter, Code, MathExpr> for DispatchEval {
    type Output = u64;

    fn compute(context: &Interpreter, code: PhantomData<Code>, expr: MathExpr) -> Self::Output {
        match expr {
            Expr::Plus(expr) => context.compute(code, expr),
            Expr::Times(expr) => context.compute(code, expr),
            Expr::Literal(expr) => context.compute(code, expr),
        }
    }
}
```

This is straightforward for a small enum like ours. But once your enum grows beyond a few variants — as is the case with something like `syn::Expr`, which contains over 40 — you quickly run into repetition, verbosity, and maintenance pain.

`MatchWithValueHandlers` avoids all that by performing this logic *generically*. It doesn't rely on macros or hardcoded pattern matching. Instead, it works entirely through traits and type-level programming. That means the same dispatcher can be reused for any enum type that satisfies the required constraints, without knowing anything about the actual enum variants ahead of time.

This is a significant benefit over traditional macro-based approaches, which are more difficult to reason about, harder to debug, and often tightly coupled to specific enum definitions. With CGP, you get a reusable, type-safe visitor implementation that scales cleanly as your codebase grows.

In short, `DispatchEval` and `MatchWithValueHandlers` together make it possible to evaluate complex enums in a clean, declarative, and extensible way — without writing repetitive boilerplate or giving up compile-time guarantees. It’s another example of how CGP turns what would normally be painful and manual trait implementations into something elegant and maintainable.

## Converting to a Lisp Expression

At this point, we’ve implemented a basic arithmetic evaluator using CGP. But interpreting expressions is only one of many possible operations we might want to perform. Often, we want to **transform** the syntax tree — say, converting it into a string, generating code, or emitting tokens for macro expansion.

Although a plain `to_string` implementation could be a compelling use case on its own, it might seem too trivial to justify CGP’s involvement (spoiler: it’s not). So instead, to make things a little more illustrative and practical, we’ll convert our arithmetic expressions into **Lisp expressions** — specifically, into a form inspired by [S-expressions](https://en.wikipedia.org/wiki/S-expression).

### Why Lisp?

The motivation here is similar to the real-world task of converting a Rust syntax tree (like [`syn::Expr`](https://docs.rs/syn/latest/syn/enum.Expr.html)) into a [`TokenStream`](https://docs.rs/proc-macro2/latest/proc_macro2/struct.TokenStream.html). That task typically requires walking a rich enum structure and transforming it into a stream of tokens. Rather than deal with the full complexity of `TokenStream`, we’ll use a simplified representation based on Lisp syntax — concise, nested, and familiar to anyone who’s seen prefix notation.

For example, our arithmetic expression `1 + (2 * 3)` would become the Lisp-like expression: `(+ 1 (* 2 3))`.

To represent this form, we define a general-purpose enum:

```rust
#[derive(HasFields, FromVariant, ExtractField)]
pub enum LispExpr {
    List(List<LispExpr>),
    Literal(Literal<u64>),
    Ident(Ident),
}

pub struct List<Expr>(pub Vec<Box<Expr>>);
pub struct Ident(pub String);
pub struct Literal<T>(pub T);
```

This `LispExpr` enum is broader than our original `Expr` — it can represent not just arithmetic but more general symbolic forms. Each `List` is a vector of boxed sub-expressions; `Literal` holds numeric values; and `Ident` wraps identifiers like `"+"` or `"*"`. For simplicity, we use a `Vec` instead of a linked list, though conceptually they’re equivalent in Lisp.

We can manually construct the equivalent of `(+ 1 (* 2 3))` like this:

```rust
let lisp_expr = LispExpr::List(List(vec![
    LispExpr::Ident(Ident("+".to_owned())).into(),
    LispExpr::Literal(Literal(1)).into(),
    LispExpr::List(List(vec![
        LispExpr::Ident(Ident("*".to_owned())).into(),
        LispExpr::Literal(Literal(2)).into(),
        LispExpr::Literal(Literal(3)).into(),
    ])).into()
]));
```

This demonstrates the basic structure. But the real point is this: converting from `Expr` to `LispExpr` **is itself another instance of the expression problem**, just like evaluation. In fact, it's even more subtle — this is a **"double expression problem"**: we want to decouple our logic from both the *source expression type* (`MathExpr`) and the *target type* (`LispExpr`).

So how do we solve it modularly?

### The `ComputerRef` Component

To implement this modular conversion, we’ll use a slightly different CGP trait: `ComputerRef`.

```rust
#[cgp_component(ComputerRef)]
pub trait CanComputeRef<Code, Input> {
    type Output;

    fn compute_ref(&self, _code: PhantomData<Code>, input: &Input) -> Self::Output;
}
```

`ComputerRef` is similar to `Computer`, but it takes a **reference** to the input rather than consuming it. This is especially useful in our case, because we might want to evaluate the expression again after transforming it — something we couldn’t do if we moved it.

While we *could* clone the input or use higher-ranked trait bounds to handle references, `ComputerRef` offers a cleaner, more ergonomic solution. CGP also provides promotion adapters that allow `ComputerRef` implementations to act as `Computer` providers when needed. So the two traits are often interchangeable in practice — use the one that fits your borrowing needs.

For the example, we used `Computer` to implement evaluation, but `ComputerRef` for to-Lisp transformation, to demonstrate the use of both traits. In practice, you might want to use `ComputerRef` for evaluation as well, so that the same expression can still be reused after evaluation.

### Implementing `PlusToLisp`

With our expression types and Lisp target representation in place, we can now implement a CGP provider that transforms a `Plus` expression into its corresponding Lisp representation. Here's what the provider looks like:

```rust
#[cgp_new_provider]
impl<Context, Code, MathExpr, LispExpr> ComputerRef<Context, Code, Plus<MathExpr>> for PlusToLisp
where
    Context:
        HasLispExprType<LispExpr = LispExpr> + CanComputeRef<Code, MathExpr, Output = LispExpr>,
    LispSubExpr<LispExpr>: CanUpcast<LispExpr>,
{
    type Output = LispExpr;

    fn compute_ref(
        context: &Context,
        code: PhantomData<Code>,
        Plus { left, right }: &Plus<MathExpr>,
    ) -> Self::Output {
        let expr_a = context.compute_ref(code, left);
        let expr_b = context.compute_ref(code, right);
        let ident = LispSubExpr::Ident(Ident("+".to_owned())).upcast(PhantomData);

        LispSubExpr::List(List(vec![ident.into(), expr_a.into(), expr_b.into()]))
            .upcast(PhantomData)
    }
}
```

This implementation takes a `Plus<MathExpr>` as input and returns a `LispExpr` as output. The transformation is recursive: each subexpression is converted by delegating to the same `ComputerRef` trait for `MathExpr`. The resulting `LispExpr` values are then combined into a list, with the `"+"` operator represented as an identifier at the head.

Notice that the provider is generic over both the context and the code. It requires that the context knows how to evaluate an `MathExpr` into a `LispExpr`, and that it defines a concrete type for `LispExpr`. This is done via a CGP type trait called `HasLispExprType`:

```rust
#[cgp_type]
pub trait HasLispExprType {
    type LispExpr;
}
```

By relying on this trait, we avoid hardcoding the `LispExpr` type directly into the provider. Instead, the actual type can be supplied later when we wire everything together.

### Constructing Variants with Sub-Enums

While we want to construct a `LispExpr` as the final result, we do not necessarily need access to all of its variants inside this provider. In fact, for converting a `Plus` node, we only need to construct two specific kinds of `LispExpr`: a `List`, and an `Ident` representing `"+"`.

To express this more precisely, we define a *local* enum called `LispSubExpr`:

```rust
#[derive(HasFields, ExtractField, FromVariant)]
enum LispSubExpr<LispExpr> {
    List(List<LispExpr>),
    Ident(Ident),
}
```

This `LispSubExpr` enum includes only the subset of variants required to construct a `Plus` expression in Lisp form. It excludes other variants like `Literal`, which may be needed by other parts of the transformation but are not relevant here. Even though `LispSubExpr` is a reduced version of `LispExpr`, it is still parameterized by the full `LispExpr` type, so that the elements in the list can recursively represent complete expressions.

To use `LispSubExpr` in our transformation, we need a way to convert — or more precisely, *upcast* — from this smaller enum into the full `LispExpr`. This is made possible by implementing the `CanUpcast` trait we [introduced earlier](#safe-enum-upcasting), which CGP derives automatically when we annotate the enum with `#[derive(HasFields, ExtractField, FromVariant)]`. This gives us a safe and type-checked way to promote the constructed value into the broader type expected by the rest of the system.

Inside the method body, we first compute the Lisp representations of the two sub-expressions. Then we create an identifier for the `"+"` symbol and upcast it to `LispExpr`. Finally, we build a `List` containing the operator followed by the two operands, and upcast that list into the final `LispExpr` result.

This pattern demonstrates how CGP’s upcasting mechanism makes it easy to construct enum values in a modular and flexible way. Instead of requiring full knowledge of the target enum’s structure, we work with a small, purpose-specific subset. This keeps our providers focused and easier to reason about, while still interoperating cleanly with the larger system.

In essence, `LispSubExpr` plays a role similar to what `#[cgp_auto_getter]` do for structs in CGP. Just as `#[cgp_auto_getter]` lets you **read** fields from a struct through a derived trait without knowing the whole type, `CanUpcast` lets you **construct** parts of an enum using only the variants you care about — without being tied to the entire definition of the enum.

### Implementing `LiteralToLisp`

The implementation of `TimesToLisp` follows the same pattern as `PlusToLisp`, differing only in that it constructs the `"*"` identifier instead of `"+"`. Since the structure is nearly identical, we will focus instead on a more interesting case: converting literal values into their Lisp representation.

The transformation of a literal is handled by the `LiteralToLisp` provider, which implements the `ComputerRef` trait. The core idea here is to wrap the literal value in a Lisp-compatible enum variant and return it as the final result. Here's the implementation:

```rust
#[cgp_new_provider]
impl<Context, Code, T, LispExpr> ComputerRef<Context, Code, Literal<T>> for LiteralToLisp
where
    Context: HasLispExprType<LispExpr = LispExpr>,
    LispSubExpr<T>: CanUpcast<LispExpr>,
    T: Clone,
{
    type Output = LispExpr;

    fn compute_ref(
        _context: &Context,
        _code: PhantomData<Code>,
        Literal(value): &Literal<T>,
    ) -> Self::Output {
        LispSubExpr::Literal(Literal(value.clone())).upcast(PhantomData)
    }
}
```

In this implementation, we pattern match on a reference to the `Literal<T>` and simply clone the value before constructing a new `Literal` variant inside a helper enum. This enum, `LispSubExpr`, plays the same role here as it did in the `PlusToLisp` provider: it defines a minimal subset of variants sufficient to perform the transformation.

```rust
#[derive(HasFields, ExtractField, FromVariant)]
enum LispSubExpr<T> {
    Literal(Literal<T>),
}
```

What makes this pattern especially powerful is that the `LispSubExpr` enum is completely parameterized over the literal type `T`. This means that the transformation logic does not need to know or care about what kind of value the literal holds. As long as `T` can be cloned, the provider works uniformly for all supported literal types — whether they are numbers, strings, or other values.

There is another subtle but important aspect to this design: the `Literal` type used here is exactly the same as the one used in our arithmetic expression tree. In other words, the same data structure is reused across both the source language (`MathExpr`) and the target language (`LispExpr`). This isn’t just a convenience — it opens the door to reusing logic across very different parts of a system.

### Wiring To-Lisp Handlers

With the Lisp transformation providers now defined, the final step is to integrate them into the interpreter context. This is where the individual pieces — evaluation, transformation, and type configuration — are all connected through CGP’s `delegate_components!` macro. To do this, we update the `InterpreterComponents` definition so that it includes the logic required for converting expressions into their Lisp representations:

```rust
#[derive(Eq, PartialEq, Debug, HasFields, FromVariant, ExtractField)]
pub enum LispExpr {
    List(List<LispExpr>),
    Literal(Literal<u64>),
    Ident(Ident),
}

delegate_components! {
    InterpreterComponents {
        LispExprTypeProviderComponent:
            UseType<LispExpr>,
        ComputerComponent:
            UseInputDelegate<
                new EvalComponents {
                    MathExpr: DispatchEval,
                    Literal<u64>: EvalLiteral,
                    Plus<Expr>: EvalAdd,
                    Times<Expr>: EvalMultiply,
                }
            >,
        ComputerRefComponent:
            UseInputDelegate<
                new ToLispComponents {
                    Expr: DispatchToLisp,
                    Literal<u64>: LiteralToLisp,
                    Plus<Expr>: PlusToLisp,
                    Times<Expr>: TimesToLisp,
                }
            >,
    }
}
```

In this setup, the `LispExprTypeProviderComponent` establishes the concrete `LispExpr` enum as the actual type behind the abstract `LispExpr` used in our providers. This mapping is done through `UseType`, which binds the type parameter required by `HasLispExprType` to the specific enum definition we want to use in the final output.

The `ComputerComponent` remains unchanged from when we configured the system for arithmetic evaluation. It continues to delegate evaluation logic to the appropriate providers, such as `EvalAdd` for addition and `EvalLiteral` for literal values.

The main addition here is the `ComputerRefComponent`, which enables reference-based computations — specifically, the transformation of expression trees into Lisp form without taking ownership of them. This component also uses `UseInputDelegate`, but it connects to a different set of providers: those responsible for generating Lisp output. It includes the transformation logic for `Plus`, `Times`, and `Literal`, each handled by their respective providers.

For the top-level `MathExpr` type, we introduce `DispatchToLisp`, a dedicated dispatcher that routes the various expression variants to their corresponding transformation providers. It is defined as follows:

```rust
#[cgp_new_provider]
impl<Code> ComputerRef<Interpreter, Code, MathExpr> for DispatchToLisp {
    type Output = LispExpr;

    fn compute_ref(context: &Interpreter, code: PhantomData<Code>, expr: &MathExpr) -> Self::Output {
        <MatchWithValueHandlersRef>::compute_ref(context, code, expr)
    }
}
```

This implementation mirrors the earlier `DispatchEval`, but with one key distinction: it uses `MatchWithValueHandlersRef`, a visitor-style dispatcher designed specifically for reference-based operations. Rather than consuming the input, it operates on borrowed values and dispatches calls to providers that implement the `ComputerRef` trait.

One of the major advantages of this approach is that it is entirely driven by the type system. Because the dispatcher is implemented generically — as a regular Rust `impl` rather than a macro — it benefits fully from the compiler’s ability to check lifetime correctness, trait bounds, and input-output consistency. Mistakes such as passing the wrong reference type, using incompatible trait bounds, or violating borrowing rules are caught immediately at compile time, often with clear and actionable error messages.

If this logic had instead been implemented using traditional Rust macros, many of these issues would only surface later during macro expansion or execution, making them harder to trace and debug. CGP’s generic dispatchers, by contrast, offer the same level of automation while remaining transparent and fully type-checked.

The `MatchWithValueHandlersRef` dispatcher is just one example of CGP’s modular dispatching infrastructure. CGP provides a *family* of such dispatchers, each tuned for a particular use case—whether by value, by reference, or with more specialized patterns. These dispatchers are designed to be extensible and interchangeable, giving you fine-grained control over how your logic is routed while preserving flexibility.

With both evaluation and Lisp transformation now wired into the same interpreter context, the system is able to evaluate expressions to numeric results or convert them into Lisp-style syntax trees, all from the same `MathExpr` type. The modularity, reusability, and compile-time guarantees of this architecture make CGP a powerful and scalable tool for building language runtimes and transformation pipelines in Rust.

## Advanced Techniques

### Binary Operator Provider

When examining the implementations of `PlusToLisp` and `TimesToLisp`, it quickly becomes clear that they follow nearly identical patterns. Aside from the specific operator symbol and the input types, the transformation logic is the same. This duplication presents a perfect opportunity for *further abstraction*.

By extracting the shared structure, we can implement a generalized provider, `BinaryOpToLisp`, that handles both `Plus` and `Times` expressions using a single implementation:

```rust
#[cgp_new_provider]
impl<Context, Code, MathExpr, MathSubExpr, LispExpr, Operator>
    ComputerRef<Context, Code, MathSubExpr> for BinaryOpToLisp<Operator>
where
    Context: HasMathExprType<MathExpr = MathExpr>
        + HasLispExprType<LispExpr = LispExpr>
        + CanComputeRef<Code, MathExpr, Output = LispExpr>,
    MathSubExpr: BinarySubExpression<MathExpr>,
    Operator: Default + Display,
    LispSubExpr<LispExpr>: CanUpcast<LispExpr>,
{
    type Output = LispExpr;

    fn compute_ref(context: &Context, code: PhantomData<Code>, expr: &MathSubExpr) -> Self::Output {
        let expr_a = context.compute_ref(code, expr.left());
        let expr_b = context.compute_ref(code, expr.right());

        let ident = LispSubExpr::Ident(Ident(Operator::default().to_string())).upcast(PhantomData);

        LispSubExpr::List(List(vec![ident.into(), expr_a.into(), expr_b.into()]))
            .upcast(PhantomData)
    }
}
```

This provider introduces a generic `Operator` type, which is expected to represent the binary operator as a type-level string, such as `"+"` or `"*"`. To support this, `Operator` must implement both `Default` and `Display`. These traits allow the provider to convert the operator type into a string during execution, which is then used to create a `LispSubExpr::Ident` variant representing the operation.

The input to this provider is any `MathSubExpr` — which could be `Plus`, `Times`, or any other binary expression type — that implements the `BinarySubExpression` trait:

```rust
#[cgp_auto_getter]
pub trait BinarySubExpression<Expr> {
    fn left(&self) -> &Box<Expr>;
    fn right(&self) -> &Box<Expr>;
}
```

By annotating this trait with `#[cgp_auto_getter]`, CGP can automatically implement it for any struct that contains `left` and `right` fields of type `Box<Expr>`. This removes the need to manually implement the trait for each binary operator type and allows the generic provider to access subexpressions in a uniform way.

To connect this trait to the right expression type, we introduce the `HasMathExprType` trait:

```rust
#[cgp_type]
pub trait HasMathExprType {
    type MathExpr;
}
```

This trait plays a similar role to `HasLispExprType`, allowing us to define the abstract `MathExpr` type outside of the generic parameters of the provider. It ensures that the right type is used consistently throughout the system and helps avoid ambiguity when specifying the generic parameter for `BinarySubExpression`.

The body of `compute_ref` mirrors the logic we saw earlier. We evaluate both the left and right subexpressions recursively, construct a Lisp identifier by calling `Operator::default().to_string()`, and then build a list containing the operator followed by the operands. The resulting Lisp structure is then upcast into the final `LispExpr` type.

With this reusable provider in place, we can now eliminate the separate implementations for `PlusToLisp` and `TimesToLisp`, and wire both operators through a single generic provider in our component configuration:

```rust
delegate_components! {
    InterpreterComponents {
        ...
        MathExprTypeProviderComponent:
            UseType<MathExpr>,
        ComputerRefComponent:
            UseInputDelegate<
                new ToLispComponents {
                    MathExpr: DispatchToLisp,
                    Literal<Value>: LiteralToLisp,
                    Plus<MathExpr>: BinaryOpToLisp<symbol!("+")>,
                    Times<MathExpr>: BinaryOpToLisp<symbol!("*")>,
                }
            >,
    }
}
```

Here, we map `Plus<MathExpr>` and `Times<MathExpr>` to the same `BinaryOpToLisp` provider, each with a different `symbol!` type-level string. The `symbol!` macro allows us to embed these operator strings as compile-time values, making the provider fully generic while preserving strong type guarantees.

Thanks to CGP’s expressive delegation system and powerful match-based dispatching via `MatchWithValueHandlersRef`, this setup allows us to write reusable, composable transformation logic. Rather than duplicating the same structure across multiple providers, we define it once in a generic form and let the type system handle the rest.

### Code-Based Dispatching

Earlier, we explored the difference between the `Computer` and `ComputerRef` traits and saw how `ComputerRef` offers a cleaner and more efficient interface for computations that don’t require ownership of the input. This naturally applies to our evaluation logic as well — after all, an evaluator only needs to borrow the expression, not consume it.

However, once we refactor our `EvalAdd`, `EvalMultiply`, and other evaluation providers to use `ComputerRef`, we run into a challenge: we’ve already wired `ComputerRefComponent` for the purpose of transforming expressions to Lisp. How do we now support *both* evaluation and transformation using the same trait?

This is where the `Code` parameter in the `ComputerRef` trait comes into play. If you’ve read about [Hypershell’s design](/blog/hypershell-release/#generic-dispatcher), you’ll recognize `Code` can be used to build type-level DSLs to encode the kind of operation we want to perform. In our interpreter, we can apply the same idea to distinguish between different computation *intentions* — for example, evaluation vs. conversion.

Let’s begin by defining two types that represent our operations at the type level:

```rust
pub struct Eval;
pub struct ToLisp;
```

These act as “statements” in our interpreter DSL. `Eval` represents program evaluation, while `ToLisp` represents conversion into Lisp syntax. This gives us a lightweight and expressive way to route logic based on the kind of computation we want to perform.

With that in place, we can define a single provider for handling `Plus` expressions, where the behavior is determined by the `Code`:

```rust
delegate_components! {
    new HandlePlus {
        ComputerRefComponent: UseDelegate<
            new PlusHandlers {
                Eval: EvalAdd,
                ToLisp: BinaryOpToLisp<symbol!("+")>,
            }>
    }
}
```

Here, we use `delegate_components!` to define a new provider called `HandlePlus`. Inside it, we delegate the `ComputerRefComponent` implementation to `UseDelegate`, which performs `Code`-based dispatching based on the newly created dispatch table `PlusHandlers`. If the `Code` is `Eval`, it uses `EvalAdd`. If it’s `ToLisp`, it uses `BinaryOpToLisp<symbol!("+")>`.

Thanks to CGP’s blanket implementations, `HandlePlus` automatically becomes a valid `ComputerRef` provider for `Plus<Expr>` — delegating to the appropriate providers based on `Code`.

We could also have achieved similar functionality by writing two separate `impl` blocks for `HandlePlus`, like this:

```rust
pub struct HandlePlus;

#[cgp_provider]
impl<Context, MathExpr> Computer<Context, Eval, Plus<MathExpr>> for HandlePlus {
    ...
}

#[cgp_provider]
impl<Context, MathExpr> ComputerRef<Context, ToLisp, Plus<MathExpr>> for HandlePlus {
    ...
}
```

This works too, but it introduces friction. Each `impl` must be written in the same crate as the type it targets (in this case, `HandlePlus`). That restricts how you organize your code. If you wanted to group all `Eval` logic into one crate and all `ToLisp` logic into another, this approach would make it more challenging to separate the implementations.

Using `delegate_components!`, on the other hand, gives you complete modularity. You can define `EvalAdd` and `BinaryOpToLisp` in completely separate places, and only compose them when building the actual interpreter.

We follow this same pattern for `Times` and `Literal`:

```rust
delegate_components! {
    new HandleTimes {
        ComputerRefComponent: UseDelegate<
            new TimesHandlers {
                Eval: EvalMultiply,
                ToLisp: BinaryOpToLisp<symbol!("*")>,
            }>
    }
}

delegate_components! {
    new HandleLiteral {
        ComputerRefComponent: UseDelegate<
            new LiteralHandlers {
                Eval: EvalLiteral,
                ToLisp: LiteralToLisp,
            }>
    }
}
```

Finally, we define a top-level dispatcher that handles the `MathExpr` enum itself:

```rust
delegate_components! {
    new HandleMathExpr {
        ComputerRefComponent: UseDelegate<
            new MathExprHandlers {
                Eval: DispatchEval,
                ToLisp: DispatchToLisp,
            }>
    }
}
```

Now that we’ve defined these composed providers, we can plug them into the interpreter’s wiring:

```rust
delegate_components! {
    InterpreterComponents {
        MathExprTypeProviderComponent:
            UseType<MathExpr>,
        LispExprTypeProviderComponent:
            UseType<LispExpr>,
        ComputerRefComponent:
            UseInputDelegate<
                new ExprComputerComponents {
                    MathExpr: HandleMathExpr,
                    Literal<u64>: HandleLiteral,
                    Plus<MathExpr>: HandlePlus,
                    Times<MathExpr>: HandleTimes,
                }
            >,
    }
}
```

At this point, we’ve created a *two-layer* dispatch system. The first layer selects a handler based on the *input type* — e.g., `Plus<MathExpr>`. The second layer selects a handler based on the *code type* — e.g., `Eval` vs. `ToLisp`.

This approach is flexible and composable. You could just as easily reverse the order, grouping logic by `Code` first and dispatching on the input type second. That may make more sense if you’re organizing your project by functionality (say, all evaluation logic in one crate, all Lisp transformation logic in another).

Importantly, the dispatch ordering is entirely compile-time and has **no impact on performance**. CGP uses Rust’s type system and monomorphization to resolve all this dispatch at compile time, so whether you dispatch by `Input` first or `Code` first, the result is the same: fast, zero-cost, strongly typed behavior.

This layered dispatch model is one of CGP’s superpowers. It enables you to write simple, focused components and compose them in flexible, scalable ways — without macros, runtime reflection, or boilerplate.

## Extending `MathExpr`

With the basic interpreter in place, supporting addition and multiplication, it’s natural to explore how we can extend the language further. To demonstrate the modularity and flexibility of CGP, let’s add two new features: *subtraction* and *negation*. These are simple but meaningful enhancements that allow us to test how well our interpreter handles incremental language growth.

Now, one could argue that subtraction and negation are not strictly necessary in the core language. After all, both operations can be expressed using multiplication by `-1`. But while this may be theoretically sound, practical language design often involves more than minimalism. By promoting these features to first-class status, we can greatly improve the ergonomics of writing and reading programs in the language.

This kind of design decision mirrors broader discussions in language evolution. Consider CGP itself. While we’ve built everything so far using CGP purely as a Rust library, it’s conceivable to imagine CGP becoming a *native* feature of the language. From a purist’s perspective, native support might not seem essential — after all, we’ve shown that powerful, compile-time generics-based programming is already achievable today. But once a tool like CGP becomes central to how systems are built, native support brings significant benefits: smoother integration, better diagnostics, and a lower learning curve.

In fact, if Rust were implemented using CGP from the start, it would be much easier to extend the language with features like CGP itself. There would be no need to fork the compiler or jump through macro-related hoops. Extensions could be introduced as structured additions to the language, just as we are now extending our interpreter with new syntax.

### Defining the `MathPlusExpr` Expression Type

To see how CGP enables modular language extension, let’s define a new expression type — `MathPlusExpr` — that expands on our original `MathExpr`. Crucially, this new enum does not replace the old one. Instead, it lives *alongside* it, allowing us to demonstrate how CGP supports language variants and extensions without duplicating logic or entangling implementations.

```rust
#[derive(Debug, HasFields, FromVariant, ExtractField)]
pub enum MathPlusExpr {
    Plus(Plus<MathPlusExpr>),
    Times(Times<MathPlusExpr>),
    Minus(Minus<MathPlusExpr>),
    Negate(Negate<MathPlusExpr>),
    Literal(Literal<i64>),
}
```

At a glance, `MathPlusExpr` looks much like `MathExpr`, but it includes two new variants: `Minus` for subtraction and `Negate` for unary negation. For the original variants — addition, multiplication, and literals — we continue to use the same generic sub-expression types as before, now instantiated with `MathPlusExpr`.

We’ve also changed the numeric type for `Literal` from `u64` to `i64`, enabling the representation of negative values. This change may seem minor, but it highlights an important feature of the system: the ability to evolve types naturally without breaking compatibility. Thanks to CGP’s generic approach, providers like `EvalAdd` and `EvalMultiply` still work seamlessly. Since `i64` also implements `Add` and `Mul`, the existing evaluators remain fully reusable without modification.

For the two new variants, we define their associated sub-expression types just as we did before:

```rust
pub struct Minus<Expr> {
    pub left: Box<Expr>,
    pub right: Box<Expr>,
}

pub struct Negate<Expr>(pub Box<Expr>);
```

### Implementing Eval Providers

With our extended expression language in place, the next step is to implement evaluation logic for the new constructs. We begin with subtraction. The evaluator for `Minus` is straightforward and closely mirrors what we’ve already done for addition and multiplication. The only real difference is that we now use the `Sub` trait to handle the subtraction operation.

```rust
#[cgp_new_provider]
impl<Context, Code, MathExpr, Output> ComputerRef<Context, Code, Minus<MathExpr>> for EvalSubtract
where
    Context: CanComputeRef<Code, MathExpr, Output = Output>,
    Output: Sub<Output = Output>,
{
    type Output = Output;

    fn compute_ref(
        context: &Context,
        code: PhantomData<Code>,
        Minus { left, right }: &Minus<MathExpr>,
    ) -> Self::Output {
        let output_a = context.compute_ref(code, left);
        let output_b = context.compute_ref(code, right);

        output_a - output_b
    }
}
```

We follow a similar pattern for the `Negate` expression. Since negation is a unary operation, its implementation is even simpler. We compute the value of the inner expression, then apply the `Neg` trait to produce the final result.

```rust
#[cgp_new_provider]
impl<Context, Code, MathExpr, Output> ComputerRef<Context, Code, Negate<MathExpr>> for EvalNegate
where
    Context: CanComputeRef<Code, MathExpr, Output = Output>,
    Output: Neg<Output = Output>,
{
    type Output = Output;

    fn compute_ref(
        context: &Context,
        code: PhantomData<Code>,
        Negate(expr): &Negate<MathExpr>,
    ) -> Self::Output {
        -context.compute_ref(code, expr)
    }
}
```

### Wiring of `InterpreterPlus`

To complete the extension, we define a new context called `InterpreterPlus`. This context wires together the evaluation logic for our extended expression language, including subtraction and negation.

```rust
#[cgp_context]
pub struct InterpreterPlus;

delegate_components! {
    InterpreterPlusComponents {
        ComputerRefComponent:
            UseDelegate<new CodeComponents {
                Eval: UseInputDelegate<new EvalComponents {
                    MathPlusExpr: DispatchEval,
                    Plus<MathPlusExpr>: EvalAdd,
                    Times<MathPlusExpr>: EvalMultiply,
                    Minus<MathPlusExpr>: EvalSubtract,
                    Negate<MathPlusExpr>: EvalNegate,
                    Literal<i64>: EvalLiteral,
                }>,
            }>
    }
}

#[cgp_new_provider]
impl ComputerRef<InterpreterPlus, Eval, MathPlusExpr> for DispatchEval {
    type Output = i64;

    fn compute_ref(
        context: &InterpreterPlus,
        code: PhantomData<Eval>,
        expr: &MathPlusExpr,
    ) -> Self::Output {
        <MatchWithValueHandlersRef>::compute_ref(context, code, expr)
    }
}
```

Thanks to CGP’s modular design, implementing `InterpreterPlus` requires only a few dozen lines of code. The core task here is to dispatch each sub-expression type to its corresponding provider. We also define a context-specific wrapper implementation that enables recursive evaluation through `MatchWithValueHandlersRef`. This approach highlights how CGP makes it easy to extend and organize language features cleanly and efficiently.

### Omitting To-Lisp Implementations

At this stage, you might assume that supporting to-Lisp conversion for `MathPlusExpr` is necessary before proceeding further. However, when rapidly prototyping new language extensions, it is often desirable to *skip* implementing less critical features like to-Lisp conversion and focus solely on the core logic, such as evaluation.

This is where CGP shines. You can choose **not to** implement certain language features — like to-Lisp conversion for `MathPlusExpr` — and still have your evaluation code compile and work perfectly without any extra effort.

This stands in stark contrast to typical Rust designs that rely on **heavyweight traits** with many methods. In those cases, introducing a new type like `MathPlusExpr` usually forces you to provide boilerplate implementations, often filled with `unimplemented!()`, just to satisfy the compiler. This can quickly become cumbersome and confusing, making it hard to know which methods are truly essential for an initial prototype.

With CGP, the minimal trait design and lazy wiring mean that components are only checked for implementation when they are actually *used*. As a result, you can safely defer adding to-Lisp conversion for `Minus` and `Negate` without worrying about subtle runtime panics or crashes caused by missing implementations.

Thanks to CGP’s flexibility and strong compile-time guarantees, once your code compiles, you can trust that missing non-essential features won’t break your core functionality — allowing you to focus on what matters most in early development.

# Under the Hood

In the **Extensible Records** section, we explored how the modular builder pattern enables us to decompose the construction of systems—such as a database client, an HTTP client, and AI agents—into independent builder providers. Similarly, in the **Extensible Variants** section, we saw how the modular visitor pattern allows us to implement evaluation and to-Lisp conversion for each variant of a language expression enum using separate visitor providers.

At this point, you’ve likely seen how these patterns can make real-world applications more modular and maintainable. If these examples have convinced you of CGP’s practical value, that’s great. But if you still feel the examples are not grounded enough in production use cases, you are welcome to pause here and revisit CGP later. The following sections are aimed at readers who want to go deeper—those interested in how CGP implements extensible data types under the hood and who might even want to contribute to CGP itself by helping to build the real-world examples you’re looking for.

## Related Work

The design of extensible data types in CGP is inspired by a rich body of research in the programming languages community. In particular, CGP draws heavily from the implementation techniques found in [**datatype-generic programming in Haskell**](https://wiki.haskell.org/index.php?title=Generics), as well as ideas presented in the paper [**Abstracting extensible data types: or, rows by any other name**](https://dl.acm.org/doi/10.1145/3290325). Concepts similar to CGP’s have also appeared in other languages under names like **row polymorphism** and **polymorphic variants**.

While it might be too academic—or simply too dry—for this post to delve into all the theoretical differences between CGP and these approaches, it’s worth highlighting one essential distinction: CGP’s seamless integration of extensible data types with the CGP system itself. This tight integration sets CGP apart by making the extensibility not just a design principle, but a native part of how you build and scale software in Rust.

## Constraint Propagation Problem

At the heart of CGP lies a powerful solution to a common challenge: how can we compose two generic functions that each rely on their own row-like constraints, and automatically propagate those constraints into the resulting composed function?

To illustrate this problem, consider the following simple example:

```rust
pub fn first_name_to_string<Context>(context: &Context) -> String
where
    Context: HasField<symbol!("first_name"), Value: Display>,
{
    context.get_field(PhantomData).to_string()
}

pub fn last_name_to_string<Context>(context: &Context) -> String
where
    Context: HasField<symbol!("last_name"), Value: Display>,
{
    context.get_field(PhantomData).to_string()
}
```

In this example, we define two functions that extract the `first_name` and `last_name` fields from a generic `Context` type and convert them to strings. The trait `HasField` is used to represent a **row constraint**, where field names are expressed as type-level strings using `symbol!`.

Now, suppose we want to combine the output of these two functions into a full name string. We can write a function that explicitly concatenates the two results, like so:

```rust
pub fn full_name_to_string<Context>(context: &Context) -> String
where
    Context: HasField<symbol!("first_name"), Value: Display>
        + HasField<symbol!("last_name"), Value: Display>,
{
    format!(
        "{} {}",
        first_name_to_string(context),
        last_name_to_string(context)
    )
}
```

This works, but requires us to manually specify all the field constraints in the function signature. This becomes increasingly cumbersome as the number of required constraints grows.

More critically, it becomes difficult to write generic higher-order functions that can accept any two such functions with their own constraints, and automatically compose them into a new function that correctly propagates those constraints. For instance, consider this naive attempt:

```rust
pub fn concate_outputs<Context>(
    fn_a: impl Fn(&Context) -> String,
    fn_b: impl Fn(&Context) -> String,
) -> impl Fn(&Context) -> String {
    move |context| format!("{} {}", fn_a(context), fn_b(context))
}
```

Here, `concate_outputs` takes two functions and returns a new one that calls both and concatenates their results. However, the function signature does not capture the constraints required by `fn_a` and `fn_b`. As a result, when we use this to build a composed function, we still have to restate all of the necessary constraints explicitly:

```rust
pub fn full_name_to_string<Context>(context: &Context) -> String
where
    Context: HasField<symbol!("first_name"), Value: Display>
        + HasField<symbol!("last_name"), Value: Display>,
{
    let composed = concate_outputs(first_name_to_string, last_name_to_string);
    composed(context)
}
```

This works within the body of a function, but it prevents us from defining the composed function as a top-level value. In other words, we cannot simply write `let full_name_to_string = concate_outputs(first_name_to_string, last_name_to_string)` and then export that directly from a module. The constraints must still be manually declared somewhere, limiting the expressiveness and reusability of our composition.

## Type-Level Composition

In many programming languages research, solving the problem of constraint-aware function composition typically requires advanced language features like **constraint kinds** or **row kinds**. These features allow constraints to be expressed and manipulated at the type level, enabling functions with different requirements to be composed seamlessly. However, languages like Rust do not yet support these advanced capabilities. This limitation has been one of the major reasons why extensible data types have not seen broader adoption in the Rust and other mainstream languages.

The breakthrough that CGP introduces is the ability to perform this kind of composition in Rust today — without waiting for the language to evolve. The key insight is to **represent functions as types**, and then use CGP's system of traits and type-level programming to manage composition and constraint propagation.

To see how this works, let’s begin by transforming two basic functions into CGP providers. Instead of writing traditional functions, we define `FirstNameToString` and `LastNameToString` as types that implement the `ComputerRef` trait:

```rust
#[cgp_new_provider]
impl<Context, Code, Input> ComputerRef<Context, Code, Input> for FirstNameToString
where
    Context: HasField<symbol!("first_name"), Value: Display>,
{
    type Output = String;

    fn compute_ref(context: &Context, _code: PhantomData<Code>, _input: &Input) -> String {
        context.get_field(PhantomData).to_string()
    }
}

#[cgp_new_provider]
impl<Context, Code, Input> ComputerRef<Context, Code, Input> for LastNameToString
where
    Context: HasField<symbol!("last_name"), Value: Display>,
{
    type Output = String;

    fn compute_ref(context: &Context, _code: PhantomData<Code>, _input: &Input) -> String {
        context.get_field(PhantomData).to_string()
    }
}
```

In this setup, `FirstNameToString` and `LastNameToString` are no longer standalone functions, but rather types that can be plugged into the CGP system. Each type implements the `ComputerRef` trait and specifies its output as a `String`. For simplicity, we ignore the `Code` and `Input` parameters, as they are not essential for this example.

Once we have our functions defined as types, we can now build a new provider that composes them:

```rust
#[cgp_new_provider]
impl<Context, Code, Input, ProviderA, ProviderB> ComputerRef<Context, Code, Input>
    for ConcatOutputs<ProviderA, ProviderB>
where
    ProviderA: ComputerRef<Context, Code, Input, Output: Display>,
    ProviderB: ComputerRef<Context, Code, Input, Output: Display>,
{
    type Output = String;

    fn compute_ref(context: &Context, code: PhantomData<Code>, input: &Input) -> String {
        let output_a = ProviderA::compute_ref(context, code, input);
        let output_b = ProviderB::compute_ref(context, code, input);
        format!("{} {}", output_a, output_b)
    }
}
```

`ConcatOutputs` acts as a **higher-order provider**: it composes two other providers and returns a new one that computes both and combines their results. This is conceptually similar to a higher-order function that takes two closures and returns a new closure, except that it operates entirely at the type level. It requires that both inner providers implement `ComputerRef` and produce outputs that implement `Display`, ensuring that both results can be formatted into a string.

The real power of this approach becomes evident when we compose the two providers with a simple type alias:

```rust
pub type FullNameToString = ConcatOutputs<FirstNameToString, LastNameToString>;
```

With this definition, `FullNameToString` behaves like a single provider that computes the full name by combining the first and last name. What’s remarkable is that we don’t need to explicitly restate the underlying constraints from `FirstNameToString` and `LastNameToString`. Those constraints are **inferred and propagated** automatically, and will only be enforced at the point where `FullNameToString` is actually used.

This programming model effectively introduces **lazy type-level computation**. The logic for computing outputs is driven by trait implementations at the type level, but the actual evaluation only occurs when the provider is invoked. This allows CGP to perform complex, constraint-aware compositions without requiring language-level support.

## `HasField` Trait

Now that we have a clearer understanding of how CGP builds on extensible data types, it is time to take a closer look at the key field-related traits that CGP introduces to support this functionality. We begin with the simplest and most foundational trait: `HasField`.

The `HasField` trait has been part of CGP since its early versions, but it is still worth revisiting here to bridge the gap toward the more advanced constructs that follow. Its definition is straightforward:

```rust
pub trait HasField<Tag> {
    type Value;

    fn get_field(&self, _tag: PhantomData<Tag>) -> &Self::Value;
}
```

This trait provides **read-only access** to individual fields of a struct, using `Tag` as the field identifier. The tag indicates which field we want to access, and can take one of two forms depending on the struct: `symbol!("first_name")` for named fields, or `Index<0>` for tuple-style fields. The associated type `Value` represents the type of the field being accessed, and the `get_field` method returns a reference to that field.

The use of `PhantomData<Tag>` as an argument may look unusual at first, but it plays a critical role in allowing Rust to infer which field is being requested. When multiple `HasField` implementations are available for a type, this allows the compiler to resolve the correct one based on context.

Consider the following struct:

```rust
#[derive(HasField)]
pub struct Person {
    pub first_name: String,
    pub last_name: String,
}
```

When the `HasField` macro is derived, it automatically generates implementations for both fields as follows.

```rust
impl HasField<symbol!("first_name")> for Person {
    type Value = String;

    fn get_field(&self, _tag: PhantomData<symbol!("first_name")>) -> &Self::Value {
        &self.first_name
    }
}

impl HasField<symbol!("last_name")> for Person {
    type Value = String;

    fn get_field(&self, _tag: PhantomData<symbol!("last_name")>) -> &Self::Value {
        &self.last_name
    }
}
```

This allows generic code to access either field in a type-safe way without requiring access to the concrete types.

With `HasField`, we can write code that reads from a subset of fields on a struct, enabling a flexible and reusable programming model. However, if we want to *construct* such subsets — as is required for the extensible builder pattern — we will first need to introduce a few additional building blocks.

## Partial Records

One of the core limitations of Rust’s struct system is that when constructing a value, you must provide values for *all* of its fields. This rigidity makes it difficult to build structs in a piecemeal fashion. To overcome this, CGP introduces the idea of **partial records** — a way to represent a struct with some fields intentionally left out.

Let’s consider the same `Person` struct used previously:

```rust
#[derive(BuildField)]
pub struct Person {
    pub first_name: String,
    pub last_name: String,
}
```

When we derive the `BuildField` trait for this struct, CGP automatically generates a corresponding partial version of the struct called `PartialPerson`:

```rust
pub struct PartialPerson<F0: MapType, F1: MapType> {
    pub first_name: F0::Mapped<String>,
    pub last_name: F1::Mapped<String>,
}
```

Here, `F0` and `F1` are type parameters that must implement the `MapType` trait. These type parameters control whether each field is present or not in a given instance of the partial struct.

The `MapType` trait is defined as follows:

```rust
pub trait MapType {
    type Mapped<T>;
}
```

This trait uses a *generic associated type* (GAT) called `Mapped` to determine how the original field types should be transformed. In particular, `Mapped<T>` will either be `T` itself (if the field is present) or a placeholder type `()` (if the field is missing).

To support this, CGP provides two implementations of the `MapType` trait. The first is `IsPresent`, which maps a type to itself to indicate that a field is included:

```rust
pub struct IsPresent;

impl MapType for IsPresent {
    type Mapped<T> = T;
}
```

The second is `IsNothing`, which maps every type to `()` to indicate that the field is absent:

```rust
pub struct IsNothing;

impl MapType for IsNothing {
    type Mapped<T> = ();
}
```

To see how this works in practice, suppose we want to construct a partial `Person` value that only includes the `first_name` field. We can instantiate the type as `PartialPerson<IsPresent, IsNothing>`, where `F0 = IsPresent` and `F1 = IsNothing`. The resulting type expands to:

```rust
pub struct PartialPerson {
    pub first_name: String,
    pub last_name: (),
}
```

This means we can create a partial instance like this:

```rust
let partial_person = PartialPerson::<IsPresent, IsNothing> {
    first_name: "John".to_owned(),
    last_name: (),
};
```

### `HasBuilder` Trait

Once we have defined a partial record struct, we can introduce the `HasBuilder` trait. This trait allows us to initialize a partial record where *all fields are absent* by default:

```rust
pub trait HasBuilder {
    type Builder;

    fn builder() -> Self::Builder;
}
```

The `HasBuilder` trait defines an associated type called `Builder`, which represents the initial form of the partial record. The key requirement is that this `Builder` must have all of its fields empty, since the `builder()` method constructs it without requiring any input.

For the `Person` struct, we can implement the `HasBuilder` trait as follows:

```rust
impl HasBuilder for Person {
    type Builder = PartialPerson<IsNothing, IsNothing>;

    fn builder() -> Self::Builder {
        PartialPerson {
            first_name: (),
            last_name: (),
        }
    }
}
```

In this implementation, the initial `Builder` is simply a `PartialPerson` type where both field parameters use the `IsNothing` type mapper. To construct the empty builder, we initialize each field with its mapped type, which for `IsNothing` is always the unit type `()`. This gives us a clean and predictable starting point for incrementally building up a complete `Person` instance.

### `BuildField` Trait

With the initial builder in place, the next step is to define the `BuildField` trait, which enables us to incrementally populate fields in the partial record. This trait is defined as follows:

```rust
pub trait BuildField<Tag> {
    type Value;
    type Output;

    fn build_field(self, _tag: PhantomData<Tag>, value: Self::Value) -> Self::Output;
}
```

The `BuildField` trait is parameterized by a `Tag` type, which identifies the field being constructed, just like in the `HasField` trait. It also includes an associated type `Value`, representing the type of the field being added.

The trait additionally includes an associated type `Output`, which represents the new type of the builder after the field has been inserted. This `Output` type is essential because each insertion updates the builder’s type parameters, effectively transforming it into a new type that reflects the presence of additional fields.

The `build_field` method takes ownership of the current builder and consumes the new field value, returning an updated builder that includes the newly added field.

To see how this works in practice, consider the implementation of `BuildField` for the `first_name` field of the `PartialPerson` struct:

```rust
impl<F1: MapType> BuildField<symbol!("first_name")> for PartialPerson<IsNothing, F1> {
    type Value = String;
    type Output = PartialPerson<IsPresent, F1>;

    fn build_field(self, _tag: PhantomData<symbol!("first_name")>, value: Self::Value) -> Self::Output {
        PartialPerson {
            first_name: value,
            last_name: self.last_name,
        }
    }
}
```

In this implementation, we define `BuildField<symbol!("first_name")>` for a `PartialPerson` where the `first_name` field is absent (`IsNothing`) and the `last_name` field is parameterized generically as `F1`. This means the implementation will work regardless of whether `last_name` is present or not. We specify the `Value` as `String`, which matches the type of `first_name`, and set the `Output` type to a new `PartialPerson` where the first parameter has been updated to `IsPresent`. The second parameter remains unchanged to preserve the existing state of `last_name`.

The method body constructs a new `PartialPerson` where the `first_name` field is now set to the given value, while the `last_name` field is carried over from the original builder.

Similarly, we can define `BuildField` for the `last_name` field:

```rust
impl<F0: MapType> BuildField<symbol!("last_name")> for PartialPerson<F0, IsNothing> {
    type Value = String;
    type Output = PartialPerson<F0, IsPresent>;

    fn build_field(self, _tag: PhantomData<symbol!("last_name")>, value: Self::Value) -> Self::Output {
        PartialPerson {
            first_name: self.first_name,
            last_name: value,
        }
    }
}
```

In this case, the generic parameter `F0` tracks the state of `first_name`, while `IsNothing` ensures that `last_name` is not yet present. The logic follows the same structure as the earlier implementation, simply updating the appropriate field.

With these implementations in place, we can now use the `HasBuilder` and `BuildField` traits together to construct a `Person` incrementally:

```rust
Person::builder() // PartialPerson<IsNothing, IsNothing>
    .build_field(PhantomData::<symbol!("first_name")>, "John".to_string()) // PartialPerson<IsPresent, IsNothing>
    .build_field(PhantomData::<symbol!("last_name")>, "Smith".to_string()) // PartialPerson<IsPresent, IsPresent>
```

Notably, the order in which fields are built does not matter. The type transformations ensure correctness regardless of sequencing, so the builder also works if we construct the `last_name` field first:

```rust
Person::builder() // PartialPerson<IsNothing, IsNothing>
    .build_field(PhantomData::<symbol!("last_name")>, "Smith".to_string()) // PartialPerson<IsNothing, IsPresent>
    .build_field(PhantomData::<symbol!("first_name")>, "John".to_string()) // PartialPerson<IsPresent, IsPresent>
```

This gives developers the freedom to build up records in any order while maintaining type safety.

### `FinalizeBuild` Trait

The previous example demonstrated how the `builder` and `build_field` methods can be used to construct values in the style of a fluent builder pattern. However, it is important to note that the result of the final `build_field` call is still a `PartialPerson<IsPresent, IsPresent>`, not a fully constructed `Person`.

This limitation arises because each `BuildField` implementation for `PartialPerson` is only responsible for inserting a single field. The presence or absence of other fields is abstracted away through generic type parameters, which means that the implementation cannot detect when the final field has been added. Consequently, there is no opportunity to directly return a `Person` value when the last required field is set.

While this might make the final construction step slightly more verbose, it is a deliberate trade-off. The generic nature of `BuildField` is what allows fields to be built in any order without having to implement every possible combination of partially constructed records — something that would otherwise result in an overwhelming combinatorial explosion of implementations.

To resolve this, CGP introduces the `FinalizeBuild` trait. This trait is used once the builder has been fully populated, converting the complete `PartialPerson` into a proper `Person` value:

```rust
pub trait FinalizeBuild {
    type Output;

    fn finalize_build(self) -> Self::Output;
}
```

The `FinalizeBuild` trait defines an associated `Output` type, representing the final constructed result. The `finalize_build` method takes ownership of the builder and transforms it into the desired output.

For `PartialPerson`, the trait is implemented as follows:

```rust
impl FinalizeBuild for PartialPerson<IsPresent, IsPresent> {
    type Output = Person;

    fn finalize_build(self) -> Self::Output {
        Person {
            first_name: self.first_name,
            last_name: self.last_name,
        }
    }
}
```

This implementation only applies when all fields are marked as present. At this point, the builder contains all the data necessary to construct a `Person`, so the conversion is a straightforward transfer of fields.

With this in place, the build process becomes complete by appending a call to `finalize_build`:

```rust
let person = Person::builder() // PartialPerson<IsNothing, IsNothing>
    .build_field(PhantomData::<symbol!("first_name")>, "John".to_string()) // PartialPerson<IsPresent, IsNothing>
    .build_field(PhantomData::<symbol!("last_name")>, "Smith".to_string()) // PartialPerson<IsPresent, IsPresent>
    .finalize_build(); // Person
```

Together, the partial record struct and the traits `HasBuilder`, `BuildField`, and `FinalizeBuild` form a solid and ergonomic foundation for supporting extensible records in CGP. All of these pieces are automatically generated by the `#[derive(BuildField)]` macro. This derive macro not only provides convenience but also ensures consistency and correctness, eliminating the possibility of compilation failures due to missing trait implementations.

## Merging of Two Partial Records

With the field builder traits in place, we can now explore how the earlier [struct building](#safe-struct-building) method `build_from` can be implemented to support merging a `Person` struct into a superset struct such as `Employee`.

Before we can implement `build_from`, we first need a few more supporting constructs to enable the merging operation.

### `IntoBuilder` Trait

In typical usage, partial records begin in an empty state and are gradually populated with field values until they can be finalized into a complete struct. However, we can also go in the opposite direction by converting a fully constructed struct *into* a partial record where all fields are present, and then progressively *remove* fields from it, one by one, until none remain.

This reverse direction is particularly important for the merging process, where we need to *move* fields out of a source struct and insert them into a target partial record, field by field.

To support this, we introduce the `IntoBuilder` trait:

```rust
pub trait IntoBuilder {
    type Builder;

    fn into_builder(self) -> Self::Builder;
}
```

This trait mirrors the structure of `HasBuilder`, with an associated `Builder` type that represents a partial record in which all fields are populated. The key difference is that `into_builder` consumes the original struct and produces its fully populated partial record equivalent.

The implementation of `IntoBuilder` for `Person` looks like this:

```rust
impl IntoBuilder for Person {
    type Builder = PartialPerson<IsPresent, IsPresent>;

    fn into_builder(self) -> Self::Builder {
        PartialPerson {
            first_name: self.first_name,
            last_name: self.last_name,
        }
    }
}
```

If you compare this to the implementation of the earlier `FinalizeBuild` trait, you’ll see that they are nearly identical in structure. The only difference is the direction of conversion — `IntoBuilder` transforms a `Person` into a `PartialPerson<IsPresent, IsPresent>`, while `FinalizeBuild` does the reverse.

Even though these implementations are symmetrical, we define `IntoBuilder` and `FinalizeBuild` as separate traits. This distinction is valuable because it makes the intent of each trait clear and prevents accidental misuse. Each trait captures a specific stage in the lifecycle of a partial record, whether we are constructing it from scratch, building it up field by field, or tearing it down for merging.

### `TakeField` Trait

Now that we have the `IntoBuilder` trait to help convert a struct into a fully populated partial record, we also need a way to extract individual fields from that partial record. This is where the `TakeField` trait comes in. It serves as the *opposite* of `BuildField`, allowing us to take a field value *out* of a partial record:

```rust
pub trait TakeField<Tag> {
    type Value;

    type Remainder;

    fn take_field(self, _tag: PhantomData<Tag>) -> (Self::Value, Self::Remainder);
}
```

The `Tag` and `Value` types in `TakeField` behave just like they do in `BuildField` and `HasField`. What is new here is the associated type `Remainder`, which represents the state of the partial record after the specified field has been removed. The `take_field` method consumes `self` and returns a tuple containing the extracted field value along with the updated remainder of the partial record.

As an example, here is the `TakeField` implementation for extracting the `first_name` field from a `PartialPerson`:

```rust
impl<F1> TakeField<symbol!("first_name")> for PartialPerson<IsPresent, F1> {
    type Value = String;
    type Remainder = PartialPerson<IsNothing, F1>;

    fn take_field(self, _tag: PhantomData<symbol!("first_name")>) -> (Self::Value, Self::Remainder) {
        let value = self.first_name;
        let remainder = PartialPerson {
            first_name: (),
            last_name: self.last_name,
        };

        (value, remainder)
    }
}
```

As shown, this implementation is defined for a `PartialPerson` where the first generic parameter is `IsPresent`, indicating that the `first_name` field is available to be taken. In the resulting `Remainder`, that parameter is updated to `IsNothing`, reflecting the removal of the field. The method itself returns the `first_name` value and a new partial record with `first_name` cleared and the rest of the fields left untouched.

This setup provides the building blocks needed to flexibly extract and transfer fields, which is essential for safely merging one struct into another.

### `HasFields` Trait

With `IntoBuilder` available, we can now begin transferring fields from a source struct into a target partial record by peeling them off one at a time. To enable this process generically, we need a mechanism to *enumerate* the fields of a struct so that generic code can discover which fields are available for transfer.

This is where the `HasFields` trait comes into play. It is defined as follows:

```rust
pub trait HasFields {
    type Fields;
}
```

The `HasFields` trait includes a single associated type called `Fields`, which holds a *type-level list* representing all the fields of the struct. For example, here is how `HasFields` would be implemented for the `Person` struct:

```rust
impl HasFields for Person {
    type Fields = Product![
        Field<symbol!("first_name"), String>,
        Field<symbol!("last_name"), String>,
    ];
}
```

Once the fields of a struct are made available as a type-level list, this list can be used to drive *type-level iteration* for field-wise operations such as merging. This lays the foundation for generically moving fields between records in a structured and type-safe way.

### `BuildFrom` Trait

The `build_from` method is defined within the `CanBuildFrom` trait, which uses a blanket implementation to support merging fields from one struct into another:

```rust
pub trait CanBuildFrom<Source> {
    type Output;

    fn build_from(self, source: Source) -> Self::Output;
}

impl<Builder, Source, Output> CanBuildFrom<Source> for Builder
where
    Source: HasFields + IntoBuilder,
    Source::Fields: FieldsBuilder<Source::Builder, Builder, Output = Output>,
{
    type Output = Output;

    fn build_from(self, source: Source) -> Output {
        Source::Fields::build_fields(source.into_builder(), self)
    }
}
```

In this setup, the `CanBuildFrom` trait is implemented for the partial record acting as the *target* of the merge, while the `Source` type is specified through a generic parameter. The trait defines an associated type `Output`, representing the result of merging the fields from `Source` into the current builder. The `build_from` method takes ownership of both the builder (`self`) and the source value, and returns a new builder that incorporates the fields from `Source`.

The blanket implementation of `CanBuildFrom` imposes several trait bounds. First, the `Source` type must implement both `HasFields` and `IntoBuilder`. These traits provide access to a type-level list of fields and the ability to convert `Source` into a partial record with all fields present. The field list obtained from `Source::Fields` must also implement the `FieldsBuilder` helper trait. This trait is responsible for driving the merge process field by field, taking as input the `Source::Builder` and the target `Builder` (i.e., `Self`), and producing an `Output`.

The implementation of `build_from` begins by converting the source into its partial form using `into_builder`, then delegates the merge logic to `FieldsBuilder::build_fields`, which handles transferring each field in sequence.

### `FieldsBuilder` Trait

The `FieldsBuilder` trait is a *private* helper used exclusively by the `CanBuildFrom` trait. It is defined as follows:

```rust
trait FieldsBuilder<Source, Target> {
    type Output;

    fn build_fields(source: Source, target: Target) -> Self::Output;
}
```

Unlike `CanBuildFrom`, the parameters for `FieldsBuilder` are slightly reordered. The `Self` type represents the list of fields from the source struct, while `Source` and `Target` refer to the partial records we are merging from and into. The goal of this trait is to drive type-level iteration across the list of fields, one by one, and move each field from the source to the target.

To accomplish this, we start with an implementation that matches on the head of the `Fields` list:

```rust
impl<Source, Target, RestFields, Tag, Value> FieldsBuilder<Source, Target>
    for Cons<Field<Tag, Value>, RestFields>
where
    Source: TakeField<Tag, Value = Value>,
    Target: BuildField<Tag, Value = Value>,
    RestFields: FieldsBuilder<Source::Remainder, Target::Output>,
{
    type Output = RestFields::Output;

    fn build_fields(source: Source, target: Target) -> Self::Output {
        let (value, next_source) = source.take_field(PhantomData);
        let next_target = target.build_field(PhantomData, value);

        RestFields::build_fields(next_source, next_target)
    }
}
```

Here, we pattern match the `Fields` type to `Cons<Field<Tag, Value>, RestFields>`, allowing us to extract the field name `Tag` and its type `Value`. Given this information, we require that the `Source` partial record implements `TakeField` and that the `Target` partial record implements `BuildField`, both using the same `Tag` and `Value`.

We then handle the remaining fields recursively by requiring `RestFields` to implement `FieldsBuilder`, using the `Remainder` type from `TakeField` as the next `Source`, and the `Output` type from `BuildField` as the next `Target`. This enables a seamless hand-off from one field to the next during the merging process.

The recursive chain is terminated by the base case, when there are no more fields left to process:

```rust
impl<Source, Target> FieldsBuilder<Source, Target> for Nil {
    type Output = Target;

    fn build_fields(_source: Source, target: Target) -> Self::Output {
        target
    }
}
```

In this final implementation, the `Source` partial record is now fully depleted — all fields have been taken out — so we simply return the `Target`, which now contains all the merged fields.

### Example Use of `BuildFrom`

The implementation of `FieldsBuilder` can appear intimidating at first, particularly for readers unfamiliar with type-level programming. To make the process more approachable, let’s walk through a concrete example using `BuildFrom` to illustrate what actually happens under the hood.

Consider a new struct named `Employee`, which contains the same fields as `Person`, along with an additional field called `employee_id`:

```rust
#[derive(BuildField)]
pub struct Employee {
    pub employee_id: u64,
    pub first_name: String,
    pub last_name: String,
}
```

We begin by constructing a `Person` value. After that, we can use `build_from` to merge its contents into a partially built `Employee`:

```rust
let person = Person {
    first_name: "John".to_owned(),
    last_name: "Smith".to_owned(),
};

let employee = Employee::builder() // PartialEmployee<IsNothing, IsNothing, IsNothing>
    .build_from(person) // PartialEmployee<IsNothing, IsPresent, IsPresent>
    .build_field(PhantomData::<symbol!("employee_id")>, 1) // PartialEmployee<IsPresent, IsPresent, IsPresent>
    .finalize_build(); // Person
```

When we call `build_from`, several steps take place behind the scenes:

* The type `PartialEmployee<IsNothing, IsNothing, IsNothing>` is required to implement `CanBuildFrom<Person>`.
  * The `Person` type implements `HasFields::Fields` as:
    ```rust
    Product![
        Field<symbol!("first_name"), String>,
        Field<symbol!("last_name"), String>,
    ]
    ```
  * `Person` also implements `IntoBuilder`, producing `PartialPerson<IsPresent, IsPresent>` as its `Builder`.
* `Person::Fields` must then implement `FieldsBuilder<PartialPerson<IsPresent, IsPresent>, PartialEmployee<IsNothing, IsNothing, IsNothing>>`.
  * The first `Cons` in the list matches:
    * `Tag` is `symbol!("first_name")`
    * `Value` is `String`
    * `RestFields` is `Cons<Field<symbol!("last_name"), String>, Nil>`
  * `PartialPerson<IsPresent, IsPresent>` implements `TakeField<symbol!("first_name"), Value = String>`, resulting in:
    * A `Remainder` of `PartialPerson<IsNothing, IsPresent>`
  * `PartialEmployee<IsNothing, IsNothing, IsNothing>` implements `BuildField<symbol!("first_name"), Value = String>`, producing:
    * An `Output` of `PartialEmployee<IsNothing, IsPresent, IsNothing>`
* The remaining fields, `Cons<Field<symbol!("last_name"), String>, Nil>`, must now implement `FieldsBuilder<PartialPerson<IsNothing, IsPresent>, PartialEmployee<IsNothing, IsPresent, IsNothing>>`.
  * This matches:
    * `Tag` is `symbol!("last_name")`
    * `Value` is `String`
    * `RestFields` is `Nil`
  * `PartialPerson<IsNothing, IsPresent>` implements `TakeField<symbol!("last_name"), Value = String>`, giving:
    * A `Remainder` of `PartialPerson<IsNothing, IsNothing>`
  * `PartialEmployee<IsNothing, IsPresent, IsNothing>` implements `BuildField<symbol!("last_name"), Value = String>`, yielding:
    * An `Output` of `PartialEmployee<IsNothing, IsPresent, IsPresent>`
* Finally, the `Nil` case implements `FieldsBuilder<PartialPerson<IsNothing, IsNothing>, PartialEmployee<IsNothing, IsPresent, IsPresent>>`, which concludes by returning:
  * `PartialEmployee<IsNothing, IsPresent, IsPresent>` as the final `Output`.

Although these steps may seem complex at first glance, a closer look reveals that the process simply moves each field from the `PartialPerson` instance into the `PartialEmployee`, one at a time. What makes this look more complicated is not the logic itself, but the fact that it is encoded entirely at the type level using traits and generics, rather than as regular Rust code.

If any part of this explanation remains unclear, it might be helpful to paste this blog post — or just this sub-section — into your favorite LLM and ask it to explain the process in simpler terms. Hopefully, the explanation provided here is already clear enough for an LLM to understand, so that it can in turn help make this pattern more accessible to developers who are still learning the intricacies of type-level programming.

## Builder Dispatcher

With the `BuildFrom` trait in place, we can now explore how CGP implements generalized **builder dispatchers** that enable flexible and reusable ways to assemble struct fields from various sources.

In earlier examples, we used a utility called `BuildAndMergeOutputs` to combine outputs from multiple builder providers such as `BuildSqliteClient`, `BuildHttpClient`, and `BuildOpenAiClient`. Under the hood, `BuildAndMergeOutputs` is built upon a more fundamental dispatcher named `BuildWithHandlers`. The implementation of this dispatcher looks like the following:

```rust
#[cgp_provider]
impl<Context, Code, Input, Output, Builder, Handlers, Res> Computer<Context, Code, Input>
    for BuildWithHandlers<Output, Handlers>
where
    Output: HasBuilder<Builder = Builder>,
    PipeHandlers<Handlers>: Computer<Context, Code, Builder, Output = Res>,
    Res: FinalizeBuild<Output = Output>,
{
    type Output = Output;

    fn compute(context: &Context, code: PhantomData<Code>, _input: Input) -> Self::Output {
        PipeHandlers::compute(context, code, Output::builder()).finalize_build()
    }
}
```

The `BuildWithHandlers` struct is parameterized by an `Output` type, which is the final struct we want to construct, and a `Handlers` type, which represents a type-level list of builder handlers. These handlers will be used to incrementally populate the fields of the `Output` struct.

This provider implements the `Computer` trait for any combination of `Context`, `Code`, and `Input`, although the `Input` parameter is intentionally ignored. To begin, the dispatcher requires the `Output` type to implement `HasBuilder`, which gives access to an initially empty partial record through `Output::builder()`.

Once the empty builder is obtained, it is passed as input to `PipeHandlers<Handlers>`, a pipeline that applies each handler in the list to progressively build up the partial record. The result of this pipeline must implement `FinalizeBuild`, allowing it to be converted into the fully constructed `Output` struct.

And yes, if you're wondering whether this `PipeHandlers` is the same one used in [Hypershell](/blog/hypershell-release) to build shell-like command pipelines — the answer is absolutely yes. `BuildWithHandlers` operates by initializing an empty partial record, passing it through a chain of handlers using `PipeHandlers`, and then finalizing the result into a complete struct. It’s the same elegant piping mechanism, just applied to a different domain.

This reuse of core abstractions like `Pipe` and `Handler` across different systems is one of the most powerful aspects of CGP. These components weren’t designed just for shell-style command execution — they were built to support general-purpose [function composition](https://en.wikipedia.org/wiki/Function_composition_%28computer_science%29), a core concept in functional programming.
